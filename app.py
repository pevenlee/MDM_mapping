import streamlit as st
import pandas as pd
import json
import time
import os
import gc
from google import genai
from google.genai import types
from rapidfuzz import process, fuzz 

# ================= 1. é…ç½®ä¸åˆå§‹åŒ– =================

st.set_page_config(page_title="LinkMed Matcher Hierarchical", layout="wide", page_icon="ğŸ§¬")

try:
    FIXED_API_KEY = st.secrets["GENAI_API_KEY"]
except:
    FIXED_API_KEY = "" 

LOCAL_MASTER_FILE = "MDM_retail.xlsx"

if 'uploader_key' not in st.session_state:
    st.session_state.uploader_key = str(time.time())
if 'final_result_df' not in st.session_state:
    st.session_state.final_result_df = None
if 'match_stats' not in st.session_state:
    st.session_state.match_stats = {}

# ================= 2. æ ¸å¿ƒå·¥å…·å‡½æ•° =================

def reset_app():
    st.session_state.final_result_df = None
    st.session_state.match_stats = {}
    st.session_state.uploader_key = str(time.time())
    st.rerun()

@st.cache_resource
def get_client():
    if not FIXED_API_KEY: return None
    return genai.Client(api_key=FIXED_API_KEY, http_options={'api_version': 'v1beta'})

def safe_generate(client, prompt, response_schema=None, retries=3):
    if client is None: return {"error": "API Key æœªé…ç½®"}
    wait_time = 2 
    for attempt in range(retries):
        try:
            config = types.GenerateContentConfig(
                response_mime_type="application/json",
                response_schema=response_schema
            )
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                contents=prompt,
                config=config
            )
            try:
                return json.loads(response.text)
            except json.JSONDecodeError:
                return {"error": "JSONè§£æå¤±è´¥", "raw": response.text}
        except Exception as e:
            if "429" in str(e) or "503" in str(e):
                if attempt < retries - 1:
                    time.sleep(wait_time * (2 ** attempt))
                    continue
            return {"error": str(e)}
    return {"error": "Max retries reached"}

@st.cache_resource(show_spinner=False)
def load_master_data():
    """åŠ è½½å¹¶å»ºç«‹ä¸¥æ ¼çš„åœ°ç†åˆ†å±‚ç´¢å¼•"""
    if os.path.exists(LOCAL_MASTER_FILE):
        try:
            gc.collect()
            if LOCAL_MASTER_FILE.endswith('.xlsx'):
                df = pd.read_excel(LOCAL_MASTER_FILE, engine='openpyxl')
            else:
                df = pd.read_csv(LOCAL_MASTER_FILE)
            
            # 1. ç´¢å¼•é‡ç½®
            df = df.reset_index(drop=True)
            
            # 2. è¡¥å…¨å¹¶æ¸…æ´—åˆ—
            target_cols = ['æ ‡å‡†åç§°', 'çœ', 'å¸‚', 'åŒº', 'æœºæ„ç±»å‹', 'åœ°å€', 'è¿é”å“ç‰Œ']
            for col in target_cols:
                if col not in df.columns: df[col] = ''
                df[col] = df[col].astype(str).replace('nan', '').str.strip()
                
            # 3. å»ºç«‹åˆ†å±‚ç´¢å¼• (Dict[Geokey, IndexObject])
            # è¿™å…è®¸æˆ‘ä»¬ç¬é—´æå–å‡º "å¹¿ä¸œçœ-å¹¿å·å¸‚-è¶Šç§€åŒº" ä¸‹çš„æ‰€æœ‰è¯åº—
            
            # ç»„åˆé”®ç´¢å¼• (æ›´ç²¾å‡†)
            # çœç´¢å¼•
            prov_groups = df.groupby('çœ').groups
            
            # å¸‚ç´¢å¼• (è€ƒè™‘åŒååŸå¸‚è¾ƒå°‘ï¼Œç›´æ¥ç”¨å¸‚åï¼Œæˆ–è€…ç”¨ çœ+å¸‚)
            # è¿™é‡Œç®€å•èµ·è§å‡è®¾å¸‚åå”¯ä¸€ï¼Œæˆ–è€…å³ä½¿é‡åä¹Ÿä¸€èµ·æœï¼Œå½±å“ä¸å¤§
            city_groups = df.groupby('å¸‚').groups
            
            # åŒºç´¢å¼• (åŒºé‡åå¤šï¼Œå¦‚â€œæœé˜³åŒºâ€ï¼Œæ‰€ä»¥æœ€å¥½æ˜¯ å¸‚+åŒºï¼Œä½†è¿™é‡Œæˆ‘ä»¬å…ˆæŒ‰åŒºåå»ºï¼Œæ£€ç´¢æ—¶å†åšäº¤é›†ä¼˜åŒ–ï¼Œæˆ–è€…ç®€å•æŒ‰åŒºå)
            # ä¸ºäº†å“åº”ç”¨æˆ·éœ€æ±‚â€œåŒä¸€ä¸ªåŒºé‡Œçš„â€ï¼Œæˆ‘ä»¬å»ºç«‹ä¸¥æ ¼çš„åŒºç´¢å¼•
            dist_groups = df.groupby('åŒº').groups
            
            # è¿é”ç´¢å¼•
            chain_groups = {}
            mask = df['è¿é”å“ç‰Œ'].str.len() > 1
            if mask.any():
                chain_groups = df[mask].groupby('è¿é”å“ç‰Œ').groups
            
            return df, prov_groups, city_groups, dist_groups, chain_groups
        except Exception as e:
            st.error(f"è¯»å–ä¸»æ•°æ®é”™è¯¯: {e}")
            return pd.DataFrame(), {}, {}, {}, {}
    else:
        return pd.DataFrame(), {}, {}, {}, {}

def smart_map_columns(client, df_user):
    user_cols = df_user.columns.tolist()
    sample_data = df_user.head(3).to_markdown(index=False)
    prompt = f"""
    åˆ†æç”¨æˆ·æ•°æ®ï¼Œæ‰¾å‡ºä»¥ä¸‹å­—æ®µå¯¹åº”çš„åˆ—åã€‚
    ç”¨æˆ·åˆ—å: {user_cols}
    é¢„è§ˆ: {sample_data}
    ä»»åŠ¡ï¼šæ‰¾å‡ºä»¥ä¸‹åˆ—ï¼ˆæ— åˆ™nullï¼‰ï¼š
    1. name_col: è¯æˆ¿åç§°
    2. chain_col: è¿é”/å“ç‰Œåç§°
    3. prov_col: çœä»½
    4. city_col: åŸå¸‚
    5. dist_col: åŒº/å¿
    6. addr_col: è¯¦ç»†åœ°å€
    è¾“å‡º JSON: {{ "name_col": "...", "chain_col": "...", "prov_col": "...", "city_col": "...", "dist_col": "...", "addr_col": "..." }}
    """
    res = safe_generate(client, prompt)
    if isinstance(res, list): res = res[0] if res else {}
    return res

def get_candidates_hierarchical(search_name, chain_name, df_master, prov_groups, city_groups, dist_groups, chain_groups, user_row, mapping):
    """
    ğŸŒŸ ä¸¥æ ¼åˆ†å±‚æ£€ç´¢é€»è¾‘ (Hierarchical Scope)
    ä¼˜å…ˆçº§: åŒº > å¸‚ > çœ > å…¨å±€
    """
    try:
        # è·å–ç”¨æˆ·è¡Œæ•°æ®
        u_prov = str(user_row[mapping['prov']]) if mapping['prov'] and pd.notna(user_row[mapping['prov']]) else ''
        u_city = str(user_row[mapping['city']]) if mapping['city'] and pd.notna(user_row[mapping['city']]) else ''
        u_dist = str(user_row[mapping['dist']]) if mapping['dist'] and pd.notna(user_row[mapping['dist']]) else ''
        
        target_indices = set()
        scope_desc = ""

        # --- å±‚çº§ 1: åŒºåŒ¹é… (District Level) ---
        if u_dist and u_dist in dist_groups:
            # æ‰¾åˆ°äº†å¯¹åº”åŒºçš„ç´¢å¼•
            # ä¼˜åŒ–: å¦‚æœæœ‰åŸå¸‚ä¿¡æ¯ï¼Œå–äº¤é›†é˜²æ­¢é‡ååŒº (ä¾‹å¦‚ä¸åŒåŸå¸‚çš„"åŸå…³åŒº")
            dist_indices = set(dist_groups[u_dist])
            
            if u_city and u_city in city_groups:
                city_indices = set(city_groups[u_city])
                # å–äº¤é›†ï¼šæ—¢åœ¨è¿™ä¸ªå¸‚ï¼Œåˆåœ¨è¿™ä¸ªåŒº
                intersection = dist_indices.intersection(city_indices)
                if intersection:
                    target_indices = intersection
                    scope_desc = f"ç²¾å‡†å®šä½: {u_city}{u_dist}"
                else:
                    # å¦‚æœäº¤é›†ä¸ºç©ºï¼ˆå¯èƒ½æ˜¯ä¸»æ•°æ®åŸå¸‚å¡«é”™äº†ï¼‰ï¼Œå›é€€åˆ°ä»…æŒ‰åŒº
                    target_indices = dist_indices
                    scope_desc = f"åŒºåŸŸå®šä½: {u_dist}"
            else:
                target_indices = dist_indices
                scope_desc = f"åŒºåŸŸå®šä½: {u_dist}"
        
        # --- å±‚çº§ 2: å¸‚åŒ¹é… (City Level) ---
        # å¦‚æœæ²¡æœ‰åŒºä¿¡æ¯ï¼Œæˆ–è€…è¯¥åŒºåœ¨ä¸»æ•°æ®é‡Œå®Œå…¨æ²¡æœ‰è®°å½•
        elif u_city and u_city in city_groups:
            target_indices = set(city_groups[u_city])
            scope_desc = f"åŸå¸‚å®šä½: {u_city}"
            
        # --- å±‚çº§ 3: çœåŒ¹é… (Province Level) ---
        elif u_prov and u_prov in prov_groups:
            target_indices = set(prov_groups[u_prov])
            scope_desc = f"çœä»½å®šä½: {u_prov}"
            
        # --- å±‚çº§ 4: å…¨å±€ (Global) ---
        else:
            target_indices = set(df_master.index)
            scope_desc = "å…¨å±€æœç´¢ (æ— åœ°ç†ä¿¡æ¯)"

        # --- è¿é”ä¸‹é’»å¢å¼º (Chain Drill-down) ---
        # å¦‚æœåœ¨ç¡®å®šçš„åœ°ç†èŒƒå›´å†…ï¼Œæˆ‘ä»¬è¿˜è¦ç‰¹åˆ«å…³æ³¨åŒè¿é”çš„åº—
        # è¿™ä¸€æ­¥æ˜¯ä¸ºäº†é˜²æ­¢æ¨¡ç³Šæœç´¢æ¼æ‰åå­—å·®å¼‚å¤§çš„åˆ†åº—
        force_chain_indices = set()
        if chain_name and chain_name in chain_groups:
            chain_indices = set(chain_groups[chain_name])
            # ä»…ä¿ç•™åœ¨å½“å‰åœ°ç†èŒƒå›´å†…çš„è¯¥è¿é”é—¨åº—
            force_chain_indices = chain_indices.intersection(target_indices)

        # --- å€™é€‰æå– ---
        candidates_indices = set()
        candidates_indices.update(force_chain_indices) # å…ˆåŠ å…¥åŒè¿é”çš„
        
        # æ¨¡ç³Šæœç´¢ (åœ¨åœ°ç†èŒƒå›´å†…)
        if target_indices:
            # ä¸ºäº†æ€§èƒ½ï¼Œå¦‚æœèŒƒå›´ä¾ç„¶å·¨å¤§ (>5000) ä¸”æœ‰è¿é”å€™é€‰ï¼Œå¯ä»¥å‡å°‘æ¨¡ç³Šæœç´¢
            # è¿™é‡Œæˆ‘ä»¬è¿˜æ˜¯åšä¸€æ¬¡æ£€ç´¢
            
            search_pool_indices = list(target_indices)
            # å®‰å…¨åˆ‡ç‰‡
            if len(search_pool_indices) > 5000 and len(force_chain_indices) > 0:
                # å¦‚æœèŒƒå›´å¤ªå¤§ä½†å·²ç»æ‰¾åˆ°äº†è¿é”åº—ï¼Œå°±åªåœ¨è¿é”åº—é‡Œæ‰¾ + å°‘é‡å…¨å±€é‡‡æ ·(è¿™é‡Œç®€åŒ–ä¸ºä¸é‡‡æ ·)
                pass 
            else:
                current_scope_df = df_master.loc[search_pool_indices]
                choices = current_scope_df['æ ‡å‡†åç§°'].fillna('').astype(str).to_dict()
                
                # æå–å‰ 8 å
                results = process.extract(search_name, choices, limit=8, scorer=fuzz.WRatio)
                for r in results:
                    candidates_indices.add(r[2])

        return list(candidates_indices), scope_desc
    
    except Exception as e:
        print(f"Hierarchical Retrieval Error: {e}")
        return [], "Error"

def ai_match_row_v3(client, user_row, search_name, chain_name, scope_desc, candidates_df):
    cols_to_keep = ['esid', 'æ ‡å‡†åç§°', 'æœºæ„ç±»å‹', 'çœ', 'å¸‚', 'åŒº', 'åœ°å€', 'è¿é”å“ç‰Œ']
    valid_cols = [c for c in cols_to_keep if c in candidates_df.columns]
    candidates_json = candidates_df[valid_cols].to_json(orient="records", force_ascii=False)
    
    prompt = f"""
    ã€è§’è‰²ã€‘ä¸»æ•°æ®åŒ¹é…ä¸“å®¶ã€‚
    
    ã€å¾…åŒ¹é…å®ä½“ã€‘
    - ç»„åˆåç§°: "{search_name}"
    - è¿é”å“ç‰Œ: "{chain_name}"
    - å½“å‰æ£€ç´¢èŒƒå›´: {scope_desc} (å·²ä»…ç­›é€‰æ­¤èŒƒå›´å†…çš„è¯åº—)
    - åŸå§‹åœ°å€: "{user_row.get('åœ°å€åˆ—_raw', '')}"
    
    ã€å€™é€‰ä¸»æ•°æ®ã€‘
    {candidates_json}
    
    ã€åŒ¹é…æ ‡å‡† - åˆ†çº§ç½®ä¿¡åº¦ã€‘:
    1. **High (é«˜)**: 
       - æ ¸å¿ƒåç§°å®Œå…¨ä¸€è‡´ æˆ– ä»…æœ‰"å¤§è¯æˆ¿/æœ‰é™å…¬å¸"ç­‰åç¼€å·®å¼‚ã€‚
       - å¹¶ä¸” åœ°å€/è¡Œæ”¿åŒºåˆ’ é«˜åº¦å»åˆã€‚
       - å¦‚æœåŒ…å«è·¯åï¼Œå¿…é¡»åŒ¹é…åˆ°ã€‚
    2. **Mid (ä¸­)**: 
       - è‚¯å®šæ˜¯åŒä¸€å®¶è¿é”ã€‚
       - ä½†åˆ†åº—åæœ‰ç»†å¾®å·®å¼‚ï¼ˆå¦‚"ä¸€åˆ†åº—" vs "ä¸€åº—"ï¼Œ"å—å±±åº—" vs "å—å±±åˆ†åº—"ï¼‰ã€‚
       - æˆ–è€…åœ°å€ä¿¡æ¯ç¼ºå¤±ï¼Œä½†è¯¥åŒºåŸŸå†…ä»…æœ‰è¿™ä¸€å®¶è¯¥å“ç‰Œçš„åº—ï¼Œé€»è¾‘ä¸Šå¤§æ¦‚ç‡æ˜¯å®ƒã€‚
    3. **Low (ä½)**: 
       - åç§°ç›¸ä¼¼ä½†æ— æ³•ç¡®å®šï¼ˆå¦‚ "åº·åº·è¯åº—" vs "åº·åº·å¤§è¯æˆ¿"ï¼Œæ— åœ°å€ä½è¯ï¼‰ã€‚
       - åªæœ‰è¿é”åä¸€è‡´ï¼Œä½†åˆ†åº—åå®Œå…¨ä¸åŒã€‚
       - æ²¡æœ‰ä»»ä½•åŒ¹é…é¡¹ã€‚
       
    ã€ç‰¹æ®Šè§„åˆ™ã€‘
    - **æ€»éƒ¨é™·é˜±**: é™¤éç”¨æˆ·æ‰¾æ€»éƒ¨ï¼Œå¦åˆ™ä¸è¦åŒ¹é…"æ€»å…¬å¸"ã€‚ä¼˜å…ˆåŒ¹é…é—¨åº—ã€‚
    
    ã€è¾“å‡º JSONã€‘:
    {{ "match_esid": "...", "match_name": "...", "match_type": "...", "confidence": "High/Mid/Low", "reason": "..." }}
    """
    return safe_generate(client, prompt)

# ================= 3. é¡µé¢ UI =================

st.markdown("""
    <style>
    .stApp {background-color: #F8F9FA;}
    .stat-card {background: #ffffff; padding: 15px; border-radius: 8px; border: 1px solid #e5e7eb; box-shadow: 0 1px 2px rgba(0,0,0,0.05);}
    .big-num {font-size: 24px; font-weight: bold; color: #1e40af;}
    .sub-text {font-size: 14px; color: #6b7280;}
    </style>
    <div style="font-size: 26px; font-weight: bold; color: #1E3A8A; margin-bottom: 20px;">
    ğŸ§¬ LinkMed Matcher (Hierarchical Logic)
    </div>
""", unsafe_allow_html=True)

client = get_client()

# åŠ è½½æ•°æ®
df_master, prov_groups, city_groups, dist_groups, chain_groups = pd.DataFrame(), {}, {}, {}, {}
if os.path.exists(LOCAL_MASTER_FILE):
    with st.spinner(f"æ­£åœ¨æ„å»ºåˆ†å±‚åœ°ç†ç´¢å¼•..."):
        df_master, prov_groups, city_groups, dist_groups, chain_groups = load_master_data()
else:
    st.warning(f"âš ï¸ æ–‡ä»¶ç¼ºå¤±: `{LOCAL_MASTER_FILE}`")

# --- Sidebar ---
with st.sidebar:
    st.header("ğŸ—„ï¸ æ§åˆ¶å°")
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºé‡ç½®", type="secondary", use_container_width=True):
        reset_app()
    if not df_master.empty:
        st.success(f"ä¸»æ•°æ®: {len(df_master)} æ¡")

# --- ä¸»æµç¨‹ ---
if st.session_state.final_result_df is None:
    st.markdown("### ğŸ“‚ 1. ä¸Šä¼ æ•°æ®")
    uploaded_file = st.file_uploader("Excel/CSV", type=['xlsx', 'csv'], key=st.session_state.uploader_key)

    if uploaded_file and not df_master.empty:
        try:
            if uploaded_file.name.endswith('.csv'): df_user = pd.read_csv(uploaded_file)
            else: df_user = pd.read_excel(uploaded_file)
        except Exception as e:
            st.error(f"æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
            st.stop()
        
        # --- 2. å­—æ®µæ˜ å°„ ---
        st.markdown("### ğŸ¤– 2. å­—æ®µæ˜ å°„")
        if 'map_config' not in st.session_state or st.session_state.get('last_file') != uploaded_file.name:
            with st.spinner("AI æ­£åœ¨åˆ†æè¡¨å¤´..."):
                st.session_state.map_config = smart_map_columns(client, df_user)
                st.session_state.last_file = uploaded_file.name
        
        map_res = st.session_state.map_config
        cols = df_user.columns.tolist()
        
        c1, c2, c3 = st.columns(3)
        def get_idx(key): return cols.index(map_res.get(key)) if map_res.get(key) in cols else 0
        
        with c1:
            col_name = st.selectbox("ğŸ“ è¯æˆ¿åç§°", cols, index=get_idx('name_col'))
            col_chain = st.selectbox("ğŸ”— è¿é”åç§°", [None]+cols, index=cols.index(map_res['chain_col'])+1 if map_res.get('chain_col') in cols else 0)
        with c2:
            col_prov = st.selectbox("ğŸ—ºï¸ çœä»½", [None]+cols, index=cols.index(map_res['prov_col'])+1 if map_res.get('prov_col') in cols else 0)
            col_city = st.selectbox("ğŸ™ï¸ åŸå¸‚", [None]+cols, index=cols.index(map_res['city_col'])+1 if map_res.get('city_col') in cols else 0)
        with c3:
            col_dist = st.selectbox("ğŸ˜ï¸ åŒºå¿", [None]+cols, index=cols.index(map_res['dist_col'])+1 if map_res.get('dist_col') in cols else 0)
            col_addr = st.selectbox("ğŸ  è¯¦ç»†åœ°å€", [None]+cols, index=cols.index(map_res['addr_col'])+1 if map_res.get('addr_col') in cols else 0)

        mapping = {'prov': col_prov, 'city': col_city, 'dist': col_dist, 'addr': col_addr, 'chain': col_chain, 'name': col_name}

        # --- 3. é¢„å¤„ç†ä¸é‡æ’ ---
        st.markdown("### âš¡ 3. åˆ†ç»„é‡æ’ä¸åŒ¹é…")
        
        # ğŸŒŸ æ ¸å¿ƒï¼šæŒ‰ç…§åœ°ç†ä½ç½®é‡æ’æ•°æ® (Regrouping)
        # è¿™æ»¡è¶³äº†â€œå°†ä¸Šä¼ çš„æ–‡ä»¶åˆ†ç»„é‡æ’â€çš„éœ€æ±‚ï¼Œä½¿å¾—å¤„ç†è¿‡ç¨‹åœ¨é€»è¾‘ä¸Šæ˜¯æŒ‰åŒºåŸŸè¿›è¡Œçš„
        sort_cols = []
        if col_prov: sort_cols.append(col_prov)
        if col_city: sort_cols.append(col_city)
        if col_dist: sort_cols.append(col_dist)
        
        if sort_cols:
            df_user_sorted = df_user.sort_values(by=sort_cols).reset_index(drop=True)
            st.caption(f"âœ… å·²æŒ‰ {sort_cols} å¯¹æ•°æ®è¿›è¡Œåˆ†ç»„é‡æ’ï¼Œå°†æŒ‰åŒºåŸŸé€å—åŒ¹é…ã€‚")
        else:
            df_user_sorted = df_user
            st.caption("âš ï¸ æœªæ£€æµ‹åˆ°åœ°ç†åˆ—ï¼Œå°†æŒ‰åŸå§‹é¡ºåºå¤„ç†ã€‚")

        # å…¨å­—åŒ¹é…å‡†å¤‡
        master_exact = df_master.drop_duplicates(subset=['æ ‡å‡†åç§°']).set_index('æ ‡å‡†åç§°').to_dict('index')
        
        exact_rows = []
        rem_indices = []
        
        # é¢„æ‰«æ
        for idx, row in df_user_sorted.iterrows():
            raw_name = str(row[col_name]).strip()
            chain_name = str(row[col_chain]).strip() if col_chain and pd.notna(row[col_chain]) else ""
            search_name = raw_name
            if chain_name and chain_name not in raw_name: search_name = f"{chain_name} {raw_name}"
            
            if search_name in master_exact:
                m = master_exact[search_name]
                r = row.to_dict()
                r.update({"åŒ¹é…ESID": m.get('esid'), "åŒ¹é…æ ‡å‡†å": search_name, "æœºæ„ç±»å‹": m.get('æœºæ„ç±»å‹'), "ç½®ä¿¡åº¦": "High", "åŒ¹é…æ–¹å¼": "å…¨å­—åŒ¹é…", "ç†ç”±": "ç²¾ç¡®å‘½ä¸­"})
                exact_rows.append(r)
            else:
                rem_indices.append(idx)
        
        df_exact = pd.DataFrame(exact_rows)
        df_rem = df_user_sorted.loc[rem_indices].copy()
        
        st.info(f"é¢„å¤„ç†å®Œæˆï¼šè‡ªåŠ¨å‘½ä¸­ {len(df_exact)} è¡Œï¼Œå‰©ä½™ {len(df_rem)} è¡Œå¾…åˆ†å±‚æ¨¡å‹åŒ¹é…ã€‚")
        
        btn_txt = f"ğŸš€ å¼€å§‹åˆ†å±‚åŒ¹é… ({len(df_rem)} è¡Œ)" if len(df_rem) > 0 else "âœ¨ ç”Ÿæˆç»“æœ"
        
        if st.button(btn_txt, type="primary"):
            ai_rows = []
            stats = {'exact': len(df_exact), 'high': 0, 'mid': 0, 'low': 0, 'no_match': 0}
            
            if len(df_rem) > 0:
                prog = st.progress(0)
                status = st.empty()
                
                for i, (orig_idx, row) in enumerate(df_rem.iterrows()):
                    try:
                        # å‡†å¤‡æ•°æ®
                        raw_name = str(row[col_name]).strip()
                        chain_name = str(row[col_chain]).strip() if col_chain and pd.notna(row[col_chain]) else ""
                        search_name = raw_name
                        if chain_name and chain_name not in raw_name: search_name = f"{chain_name} {raw_name}"
                        
                        # ä¼ é€’åŸå§‹åœ°å€ç»™ Prompt åšè¾…åŠ©
                        row_with_meta = row.copy()
                        if col_addr: row_with_meta['åœ°å€åˆ—_raw'] = str(row[col_addr])

                        # ğŸŒŸ è°ƒç”¨åˆ†å±‚æ£€ç´¢ (Hierarchical)
                        indices, scope_desc = get_candidates_hierarchical(
                            search_name, chain_name, df_master, 
                            prov_groups, city_groups, dist_groups, chain_groups, 
                            row, mapping
                        )
                        
                        base_res = row.to_dict()
                        if not indices:
                            base_res.update({"åŒ¹é…ESID": None, "åŒ¹é…æ ‡å‡†å": None, "æœºæ„ç±»å‹": None, "ç½®ä¿¡åº¦": "Low", "åŒ¹é…æ–¹å¼": "æ— ç»“æœ", "ç†ç”±": f"èŒƒå›´[{scope_desc}]å†…æ— å€™é€‰"})
                            stats['no_match'] += 1
                        else:
                            try:
                                candidates = df_master.loc[indices].copy()
                            except:
                                candidates = pd.DataFrame()

                            if candidates.empty:
                                base_res.update({"åŒ¹é…ESID": None, "åŒ¹é…æ ‡å‡†å": None, "æœºæ„ç±»å‹": None, "ç½®ä¿¡åº¦": "Low", "åŒ¹é…æ–¹å¼": "æ— ç»“æœ", "ç†ç”±": "ç´¢å¼•å¼‚å¸¸"})
                                stats['no_match'] += 1
                            else:
                                # ğŸŒŸ è°ƒç”¨ V3 Prompt (High/Mid/Low)
                                ai_res = ai_match_row_v3(client, row_with_meta, search_name, chain_name, scope_desc, candidates)
                                if isinstance(ai_res, list): ai_res = ai_res[0] if ai_res else {}
                                
                                conf = ai_res.get("confidence", "Low")
                                base_res.update({
                                    "åŒ¹é…ESID": ai_res.get("match_esid"),
                                    "åŒ¹é…æ ‡å‡†å": ai_res.get("match_name"),
                                    "æœºæ„ç±»å‹": ai_res.get("match_type"),
                                    "ç½®ä¿¡åº¦": conf,
                                    "åŒ¹é…æ–¹å¼": f"æ¨¡å‹ ({scope_desc})",
                                    "ç†ç”±": ai_res.get("reason")
                                })
                                
                                if conf == "High": stats['high'] += 1
                                elif conf == "Mid": stats['mid'] += 1
                                else: stats['low'] += 1
                                
                                time.sleep(1.5)
                        
                        ai_rows.append(base_res)
                        prog.progress((i+1)/len(df_rem))
                        status.text(f"[{scope_desc}] Processing: {search_name}")
                        
                    except Exception as e:
                        st.warning(f"è·³è¿‡è¡Œ: {e}")
            
            # åˆå¹¶ç»“æœ
            if ai_rows:
                df_ai = pd.DataFrame(ai_rows)
                df_final = pd.concat([df_exact, df_ai], ignore_index=True)
            else:
                df_final = df_exact
            
            st.session_state.final_result_df = df_final
            st.session_state.match_stats = stats
            st.rerun()

# --- 4. ç»“æœå±•ç¤º ---
if st.session_state.final_result_df is not None:
    s = st.session_state.match_stats
    total = s.get('total', 0)
    if total == 0: total = len(st.session_state.final_result_df)
    if total == 0: total = 1
    
    st.markdown("### ğŸ“Š åŒ¹é…ç»Ÿè®¡æŠ¥å‘Š")
    
    # æå‰è®¡ç®—æ¯”ç‡
    exact_val = s.get('exact', 0)
    model_done = s.get('high', 0) + s.get('mid', 0) + s.get('low', 0)
    
    # é˜²æ­¢åˆ†æ¯ä¸º0
    model_denom = model_done if model_done > 0 else 1
    
    col1, col2, col3, col4, col5 = st.columns(5)
    
    with col1:
        st.markdown(f"""
        <div class="stat-card">
            <div class="sub-text">ğŸ¯ å…¨å­—åŒ¹é…</div>
            <div class="big-num">{exact_val}</div>
            <div style="color:green; font-weight:bold;">{exact_val/total:.1%}</div>
        </div>""", unsafe_allow_html=True)
    with col2:
        st.markdown(f"""
        <div class="stat-card">
            <div class="sub-text">ğŸ¤– æ¨¡å‹æ€»è®¡</div>
            <div class="big-num">{model_done}</div>
            <div style="color:blue; font-weight:bold;">{model_done/total:.1%}</div>
        </div>""", unsafe_allow_html=True)
    with col3:
        h_val = s.get('high', 0)
        st.markdown(f"""
        <div class="stat-card">
            <div class="sub-text">ğŸ”¥ High</div>
            <div class="big-num">{h_val}</div>
            <div class="sub-text">å æ¨¡å‹: {h_val/model_denom:.1%}</div>
        </div>""", unsafe_allow_html=True)
    with col4:
        m_val = s.get('mid', 0)
        st.markdown(f"""
        <div class="stat-card">
            <div class="sub-text">âš–ï¸ Mid</div>
            <div class="big-num">{m_val}</div>
            <div class="sub-text">å æ¨¡å‹: {m_val/model_denom:.1%}</div>
        </div>""", unsafe_allow_html=True)
    with col5:
        l_val = s.get('low', 0)
        st.markdown(f"""
        <div class="stat-card">
            <div class="sub-text">âš ï¸ Low</div>
            <div class="big-num">{l_val}</div>
            <div class="sub-text">å æ¨¡å‹: {l_val/model_denom:.1%}</div>
        </div>""", unsafe_allow_html=True)

    st.divider()
    
    def color_row(row):
        conf = row.get('ç½®ä¿¡åº¦')
        if conf == 'High': return ['background-color: #dcfce7'] * len(row) # ç»¿
        if conf == 'Mid': return ['background-color: #fef9c3'] * len(row)  # é»„
        if conf == 'Low': return ['background-color: #fee2e2'] * len(row)  # çº¢
        return [''] * len(row)

    df_show = st.session_state.final_result_df
    st.dataframe(df_show.style.apply(color_row, axis=1), use_container_width=True)
    
    csv = df_show.to_csv(index=False).encode('utf-8-sig')
    st.download_button("ğŸ“¥ ä¸‹è½½ç»“æœ (å« High/Mid/Low)", csv, "linkmed_hierarchical.csv", "text/csv", type="primary")
