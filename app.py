import streamlit as st
import pandas as pd
import json
import time
import os
import gc
import math
from google import genai
from google.genai import types
from rapidfuzz import process, fuzz 

# ================= 1. é…ç½®ä¸åˆå§‹åŒ– =================

st.set_page_config(page_title="LinkMed Matcher Pro (Fast Batch)", layout="wide", page_icon="ğŸ§¬")

try:
    FIXED_API_KEY = st.secrets["GENAI_API_KEY"]
except:
    FIXED_API_KEY = "" 

LOCAL_MASTER_FILE = "MDM_retail.xlsx"

# åˆå§‹åŒ– Session State
if 'uploader_key' not in st.session_state:
    st.session_state.uploader_key = str(time.time())
if 'final_result_df' not in st.session_state:
    st.session_state.final_result_df = None
if 'match_stats' not in st.session_state:
    st.session_state.match_stats = {}
# ç”¨äºå­˜å‚¨æ‹†åˆ†å¥½çš„ä»»åŠ¡çŠ¶æ€
if 'prep_done' not in st.session_state:
    st.session_state.prep_done = False
if 'df_exact' not in st.session_state:
    st.session_state.df_exact = None
if 'batches' not in st.session_state:
    st.session_state.batches = []
if 'total_rem' not in st.session_state:
    st.session_state.total_rem = 0

# ================= 2. æ ¸å¿ƒå·¥å…·å‡½æ•° =================

def reset_app():
    """å®Œå…¨é‡ç½®"""
    st.session_state.final_result_df = None
    st.session_state.match_stats = {}
    st.session_state.prep_done = False
    st.session_state.df_exact = None
    st.session_state.batches = []
    st.session_state.total_rem = 0
    st.session_state.uploader_key = str(time.time())
    st.rerun()

@st.cache_resource
def get_client():
    if not FIXED_API_KEY: return None
    return genai.Client(api_key=FIXED_API_KEY, http_options={'api_version': 'v1beta'})

def safe_generate(client, prompt, response_schema=None, retries=3):
    if client is None: return {"error": "API Key æœªé…ç½®"}
    wait_time = 2 
    for attempt in range(retries):
        try:
            config = types.GenerateContentConfig(
                response_mime_type="application/json",
                response_schema=response_schema
            )
            response = client.models.generate_content(
                model="gemini-3-flash", 
                contents=prompt,
                config=config
            )
            try:
                return json.loads(response.text)
            except json.JSONDecodeError:
                return {"error": "JSONè§£æå¤±è´¥", "raw": response.text}
        except Exception as e:
            if "429" in str(e) or "503" in str(e):
                if attempt < retries - 1:
                    time.sleep(wait_time * (2 ** attempt))
                    continue
            return {"error": str(e)}
    return {"error": "Max retries reached"}

@st.cache_resource(show_spinner=False)
def load_master_data():
    if os.path.exists(LOCAL_MASTER_FILE):
        try:
            gc.collect()
            if LOCAL_MASTER_FILE.endswith('.xlsx'):
                df = pd.read_excel(LOCAL_MASTER_FILE, engine='openpyxl')
            else:
                df = pd.read_csv(LOCAL_MASTER_FILE)
            
            df = df.reset_index(drop=True)
            target_cols = ['æ ‡å‡†åç§°', 'çœ', 'å¸‚', 'åŒº', 'æœºæ„ç±»å‹', 'åœ°å€', 'è¿é”å“ç‰Œ']
            for col in target_cols:
                if col not in df.columns: df[col] = ''
                df[col] = df[col].astype(str).replace('nan', '').str.strip()
            
            # å»ºç«‹ç´¢å¼•
            prov_groups = df.groupby('çœ').groups
            city_groups = df.groupby('å¸‚').groups
            dist_groups = df.groupby('åŒº').groups
            
            chain_groups = {}
            mask = df['è¿é”å“ç‰Œ'].str.len() > 1
            if mask.any():
                chain_groups = df[mask].groupby('è¿é”å“ç‰Œ').groups
            
            return df, prov_groups, city_groups, dist_groups, chain_groups
        except Exception as e:
            st.error(f"è¯»å–ä¸»æ•°æ®é”™è¯¯: {e}")
            return pd.DataFrame(), {}, {}, {}, {}
    else:
        return pd.DataFrame(), {}, {}, {}, {}

def smart_map_columns(client, df_user):
    user_cols = df_user.columns.tolist()
    sample_data = df_user.head(3).to_markdown(index=False)
    prompt = f"""
    åˆ†æç”¨æˆ·æ•°æ®ï¼Œæ‰¾å‡ºä»¥ä¸‹å­—æ®µå¯¹åº”çš„åˆ—åï¼ˆå¯èƒ½æ¶‰åŠä¸­è‹±æ–‡è½¬åŒ–ï¼‰ã€‚
    ç”¨æˆ·åˆ—å: {user_cols}
    é¢„è§ˆ: {sample_data}
    ä»»åŠ¡ï¼šæ‰¾å‡ºä»¥ä¸‹åˆ—ï¼ˆæ— åˆ™nullï¼‰ï¼š
    1. name_col: è¯æˆ¿åç§°
    2. chain_col: è¿é”/å“ç‰Œåç§°
    3. prov_col: çœä»½
    4. city_col: åŸå¸‚
    5. dist_col: åŒº/å¿
    6. addr_col: è¯¦ç»†åœ°å€
    è¾“å‡º JSON: {{ "name_col": "...", "chain_col": "...", "prov_col": "...", "city_col": "...", "dist_col": "...", "addr_col": "..." }}
    """
    res = safe_generate(client, prompt)
    if isinstance(res, list): res = res[0] if res else {}
    return res

def get_candidates_hierarchical(search_name, chain_name, df_master, prov_groups, city_groups, dist_groups, chain_groups, user_row, mapping):
    try:
        u_prov = str(user_row[mapping['prov']]) if mapping['prov'] and pd.notna(user_row[mapping['prov']]) else ''
        u_city = str(user_row[mapping['city']]) if mapping['city'] and pd.notna(user_row[mapping['city']]) else ''
        u_dist = str(user_row[mapping['dist']]) if mapping['dist'] and pd.notna(user_row[mapping['dist']]) else ''
        
        target_indices = set()
        scope_desc = ""

        if u_dist and u_dist in dist_groups:
            dist_indices = set(dist_groups[u_dist])
            if u_city and u_city in city_groups:
                city_indices = set(city_groups[u_city])
                intersection = dist_indices.intersection(city_indices)
                target_indices = intersection if intersection else dist_indices
                scope_desc = f"ç²¾å‡†å®šä½: {u_city}{u_dist}"
            else:
                target_indices = dist_indices
                scope_desc = f"åŒºåŸŸå®šä½: {u_dist}"
        
        elif u_city and u_city in city_groups:
            target_indices = set(city_groups[u_city])
            scope_desc = f"åŸå¸‚å®šä½: {u_city}"
            
        elif u_prov and u_prov in prov_groups:
            target_indices = set(prov_groups[u_prov])
            scope_desc = f"çœä»½å®šä½: {u_prov}"
            
        else:
            target_indices = set(df_master.index)
            scope_desc = "å…¨å±€æœç´¢"

        force_chain_indices = set()
        if chain_name and chain_name in chain_groups:
            chain_indices = set(chain_groups[chain_name])
            force_chain_indices = chain_indices.intersection(target_indices)

        candidates_indices = set()
        candidates_indices.update(force_chain_indices) 
        
        if target_indices:
            search_pool_indices = list(target_indices)
            if len(search_pool_indices) > 5000 and len(force_chain_indices) > 0:
                 search_pool_indices = search_pool_indices[:2000] 

            current_scope_df = df_master.loc[search_pool_indices]
            choices = current_scope_df['æ ‡å‡†åç§°'].fillna('').astype(str).to_dict()
            
            results = process.extract(search_name, choices, limit=8, scorer=fuzz.WRatio)
            for r in results:
                candidates_indices.add(r[2])

        return list(candidates_indices), scope_desc
    
    except Exception as e:
        return [], "Error"

def ai_match_row_v3(client, user_row, search_name, chain_name, scope_desc, candidates_df):
    cols_to_keep = ['esid', 'æ ‡å‡†åç§°', 'æœºæ„ç±»å‹', 'çœ', 'å¸‚', 'åŒº', 'åœ°å€', 'è¿é”å“ç‰Œ']
    valid_cols = [c for c in cols_to_keep if c in candidates_df.columns]
    candidates_json = candidates_df[valid_cols].to_json(orient="records", force_ascii=False)
    
    prompt = f"""
    ã€è§’è‰²ã€‘ä¸»æ•°æ®åŒ¹é…ä¸“å®¶ã€‚
    ã€å¾…åŒ¹é…å®ä½“ã€‘
    - ç»„åˆåç§°: "{search_name}"
    - è¿é”å“ç‰Œ: "{chain_name}"
    - æ£€ç´¢èŒƒå›´: {scope_desc}
    - åŸå§‹åœ°å€: "{user_row.get('åœ°å€åˆ—_raw', '')}"
    
    ã€å€™é€‰ä¸»æ•°æ®ã€‘
    {candidates_json}
    
    ã€åŒ¹é…æ ‡å‡† - åˆ†çº§ç½®ä¿¡åº¦ã€‘:
    1. **High**: æ ¸å¿ƒåç§°ä¸€è‡´ä¸”åœ°å€/è¡Œæ”¿åŒºå»åˆã€‚
    2. **Mid**: æ˜¯åŒä¸€è¿é”ï¼Œä½†åˆ†åº—åæœ‰ç»†å¾®å·®å¼‚ï¼Œæˆ–åœ°å€ç¼ºå¤±ä½†åŒºåŸŸå†…ä»…æ­¤ä¸€å®¶ã€‚
    3. **Low**: åç§°ç›¸ä¼¼æ— æ³•ç¡®å®šï¼Œæˆ–åªæœ‰è¿é”åä¸€è‡´åˆ†åº—ä¸åŒã€‚
       
    ã€ç‰¹æ®Šè§„åˆ™ã€‘
    - **æ€»éƒ¨é™·é˜±**: é™¤éç”¨æˆ·æ‰¾æ€»éƒ¨ï¼Œå¦åˆ™ä¸è¦åŒ¹é…"æ€»å…¬å¸"ã€‚ä¼˜å…ˆåŒ¹é…é—¨åº—ã€‚
    - å½“åŒ¹é…ç»“æœä¸ºlowï¼Œé€šè¿‡è¯åº—ä¿¡æ¯ä¸­çš„ XXåº—ï¼Œå»ä¸»æ•°æ®çš„åœ°å€ä¸­å¯»æ‰¾ï¼Œå¦‚æœä¸»æ•°æ®ä¸­çš„åœ°å€åŒ…å«XXï¼Œåˆ™æ¨¡ç³ŠåŒ¹é…ä¸Š
    
    ã€è¾“å‡º JSONã€‘:
    {{ "match_esid": "...", "match_name": "...", "match_type": "...", "confidence": "High/Mid/Low", "reason": "..." }}
    """
    return safe_generate(client, prompt)

# ================= 3. é¡µé¢ UI =================

st.markdown("""
    <style>
    .stApp {background-color: #F8F9FA;}
    .stat-card {background: #ffffff; padding: 15px; border-radius: 8px; border: 1px solid #e5e7eb; box-shadow: 0 1px 2px rgba(0,0,0,0.05);}
    .big-num {font-size: 24px; font-weight: bold; color: #1e40af;}
    .sub-text {font-size: 14px; color: #6b7280;}
    .task-box {background-color: #eff6ff; padding: 12px; border-radius: 6px; margin-bottom: 8px; border-left: 5px solid #2563eb; font-size:14px;}
    .prog-label {font-weight: bold; font-size: 14px; margin-bottom: 5px; display: block;}
    </style>
    <div style="font-size: 26px; font-weight: bold; color: #1E3A8A; margin-bottom: 20px;">
    ğŸ§¬ LinkMed Matcher (Stable Batch)
    </div>
""", unsafe_allow_html=True)

client = get_client()

# åŠ è½½æ•°æ®
df_master, prov_groups, city_groups, dist_groups, chain_groups = pd.DataFrame(), {}, {}, {}, {}
if os.path.exists(LOCAL_MASTER_FILE):
    with st.spinner(f"æ­£åœ¨åŠ è½½ä¸»æ•°æ®..."):
        df_master, prov_groups, city_groups, dist_groups, chain_groups = load_master_data()
else:
    st.warning(f"âš ï¸ æ–‡ä»¶ç¼ºå¤±: `{LOCAL_MASTER_FILE}`")

# --- Sidebar ---
with st.sidebar:
    st.header("ğŸ—„ï¸ æ§åˆ¶å°")
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºé‡ç½®", type="secondary", use_container_width=True):
        reset_app()
    if not df_master.empty:
        st.success(f"ä¸»æ•°æ®: {len(df_master)} æ¡")

# --- ä¸»æµç¨‹ ---
if st.session_state.final_result_df is None:
    st.markdown("### ğŸ“‚ 1. ä¸Šä¼ æ•°æ®")
    uploaded_file = st.file_uploader("Excel/CSV", type=['xlsx', 'csv'], key=st.session_state.uploader_key)

    if uploaded_file and not df_master.empty:
        try:
            if uploaded_file.name.endswith('.csv'): df_user = pd.read_csv(uploaded_file)
            else: df_user = pd.read_excel(uploaded_file)
        except Exception as e:
            st.error(f"æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
            st.stop()
        
        # --- 2. å­—æ®µæ˜ å°„ ---
        st.markdown("### ğŸ¤– 2. å­—æ®µæ˜ å°„")
        # è‡ªåŠ¨æ˜ å°„ä»…è¿è¡Œä¸€æ¬¡
        if 'map_config' not in st.session_state or st.session_state.get('last_file') != uploaded_file.name:
            st.session_state.prep_done = False # é‡ç½®é¢„å¤„ç†çŠ¶æ€
            with st.spinner("AI æ­£åœ¨åˆ†æè¡¨å¤´..."):
                st.session_state.map_config = smart_map_columns(client, df_user)
                st.session_state.last_file = uploaded_file.name
        
        map_res = st.session_state.map_config
        cols = df_user.columns.tolist()
        
        c1, c2, c3 = st.columns(3)
        def get_idx(key): return cols.index(map_res.get(key)) if map_res.get(key) in cols else 0
        
        with c1:
            col_name = st.selectbox("ğŸ“ è¯æˆ¿åç§°", cols, index=get_idx('name_col'))
            col_chain = st.selectbox("ğŸ”— è¿é”åç§°", [None]+cols, index=cols.index(map_res['chain_col'])+1 if map_res.get('chain_col') in cols else 0)
        with c2:
            col_prov = st.selectbox("ğŸ—ºï¸ çœä»½", [None]+cols, index=cols.index(map_res['prov_col'])+1 if map_res.get('prov_col') in cols else 0)
            col_city = st.selectbox("ğŸ™ï¸ åŸå¸‚", [None]+cols, index=cols.index(map_res['city_col'])+1 if map_res.get('city_col') in cols else 0)
        with c3:
            col_dist = st.selectbox("ğŸ˜ï¸ åŒºå¿", [None]+cols, index=cols.index(map_res['dist_col'])+1 if map_res.get('dist_col') in cols else 0)
            col_addr = st.selectbox("ğŸ  è¯¦ç»†åœ°å€", [None]+cols, index=cols.index(map_res['addr_col'])+1 if map_res.get('addr_col') in cols else 0)

        mapping = {'prov': col_prov, 'city': col_city, 'dist': col_dist, 'addr': col_addr, 'chain': col_chain, 'name': col_name}

        # --- 3. é¢„å¤„ç†ä¸åˆ†åŒ… ---
        st.markdown("### âš¡ 3. é¢„å¤„ç†ä¸åˆ†åŒ…")
        
        if not st.session_state.prep_done:
            if st.button("ğŸ å¼€å§‹é¢„å¤„ç†åˆ†æ", type="primary"):
                with st.spinner("æ­£åœ¨è¿›è¡Œæé€Ÿåˆ†æä¸æ‹†åŒ…..."):
                    try:
                        # 1. å®‰å…¨æ•°æ®æ¸…æ´—
                        df_safe = df_user.copy()
                        for c in [col_name, col_chain, col_prov, col_city, col_dist, col_addr]:
                            if c:
                                df_safe[c] = df_safe[c].astype(str).replace('nan', '').str.strip()
                        
                        # 2. åœ°ç†æ’åº
                        sort_cols = []
                        if col_prov: sort_cols.append(col_prov)
                        if col_city: sort_cols.append(col_city)
                        if col_dist: sort_cols.append(col_dist)
                        if sort_cols:
                            df_safe = df_safe.sort_values(by=sort_cols).reset_index(drop=True)

                        # 3. å‘é‡åŒ–å…¨å­—åŒ¹é… (Vectorized Exact Match)
                        master_exact = df_master.drop_duplicates(subset=['æ ‡å‡†åç§°']).set_index('æ ‡å‡†åç§°').to_dict('index')
                        
                        def check_exact(row):
                            raw = row[col_name]
                            chain = row[col_chain] if col_chain else ""
                            search = raw
                            if chain and chain not in raw: search = f"{chain} {raw}"
                            
                            if search in master_exact:
                                m = master_exact[search]
                                return pd.Series([
                                    True, m.get('esid'), search, m.get('æœºæ„ç±»å‹'), "High", "å…¨å­—åŒ¹é…", "ç²¾ç¡®å‘½ä¸­"
                                ])
                            return pd.Series([False, None, None, None, None, None, None])

                        # æ‰¹é‡åº”ç”¨é€»è¾‘
                        match_results = df_safe.apply(check_exact, axis=1)
                        match_results.columns = ['is_match', 'åŒ¹é…ESID', 'åŒ¹é…æ ‡å‡†å', 'æœºæ„ç±»å‹', 'ç½®ä¿¡åº¦', 'åŒ¹é…æ–¹å¼', 'ç†ç”±']
                        
                        # åˆå¹¶ç»“æœ
                        df_combined = pd.concat([df_safe, match_results], axis=1)
                        
                        # æ‹†åˆ†ç»“æœ
                        df_exact = df_combined[df_combined['is_match'] == True].drop(columns=['is_match'])
                        df_rem = df_combined[df_combined['is_match'] == False].drop(columns=['is_match', 'åŒ¹é…ESID', 'åŒ¹é…æ ‡å‡†å', 'æœºæ„ç±»å‹', 'ç½®ä¿¡åº¦', 'åŒ¹é…æ–¹å¼', 'ç†ç”±'])
                        
                        # å­˜å…¥ Session
                        st.session_state.df_exact = df_exact
                        st.session_state.total_rem = len(df_rem)
                        
                        # 4. æ‹†åˆ†æ‰¹æ¬¡
                        batches = []
                        if len(df_rem) > 0:
                            BATCH_SIZE = 1000 
                            num_batches = math.ceil(len(df_rem) / BATCH_SIZE)
                            for i in range(num_batches):
                                batches.append(df_rem.iloc[i*BATCH_SIZE : (i+1)*BATCH_SIZE])
                        
                        st.session_state.batches = batches
                        st.session_state.prep_done = True
                        st.rerun()
                        
                    except Exception as e:
                        st.error(f"é¢„å¤„ç†å‘ç”Ÿé”™è¯¯: {e}")
                        st.stop()
        
        # --- æ¸²æŸ“ä»»åŠ¡åˆ—è¡¨ ---
        if st.session_state.prep_done:
            count_exact = len(st.session_state.df_exact)
            count_rem = st.session_state.total_rem
            batches = st.session_state.batches
            
            st.info(f"âœ… é¢„å¤„ç†å®Œæˆï¼šè‡ªåŠ¨å‘½ä¸­ {count_exact} è¡Œã€‚å‰©ä½™ {count_rem} è¡Œå¾…æ¨¡å‹åŒ¹é…ã€‚")
            
            if count_rem > 0:
                st.markdown(f"**å·²æ‹†åˆ†ä¸º {len(batches)} ä¸ªä»»åŠ¡åŒ…ï¼ˆæ¯åŒ…çº¦ 1000 æ¡ï¼‰ï¼Œé˜²æ­¢å†…å­˜æº¢å‡ºã€‚**")
                
                # å¯æŠ˜å çš„ä»»åŠ¡é¢„è§ˆ
                with st.expander(f"ğŸ‘ï¸ æŸ¥çœ‹ {len(batches)} ä¸ªä»»åŠ¡åŒ…è¯¦æƒ…", expanded=False):
                    for i, b in enumerate(batches):
                        tag = "æ··åˆåŒºåŸŸ"
                        if len(b) > 0:
                            r = b.iloc[0]
                            p = r[col_prov] if col_prov else ""
                            c = r[col_city] if col_city else ""
                            if p or c: tag = f"{p} {c}"
                        st.markdown(f"<div class='task-box'>ğŸ“¦ <b>ä»»åŠ¡åŒ… {i+1}</b>: {len(b)} è¡Œ <small>({tag})</small></div>", unsafe_allow_html=True)
                
                # å¯åŠ¨æŒ‰é’®
                if st.button(f"ğŸš€ å¯åŠ¨ä»»åŠ¡é˜Ÿåˆ— ({count_rem} è¡Œ)", type="primary"):
                    
                    final_accumulated = st.session_state.df_exact.copy()
                    stats = {'exact': count_exact, 'high': 0, 'mid': 0, 'low': 0, 'no_match': 0}
                    
                    st.write("") 
                    col_g, col_b = st.columns(2)
                    with col_g:
                        st.markdown('<span class="prog-label">ğŸŒ å…¨å±€æ€»è¿›åº¦</span>', unsafe_allow_html=True)
                        global_prog = st.progress(0)
                        global_txt = st.empty()
                    
                    with col_b:
                        st.markdown('<span class="prog-label">ğŸ“¦ å½“å‰ä»»åŠ¡åŒ…è¿›åº¦</span>', unsafe_allow_html=True)
                        batch_prog = st.progress(0)
                        batch_txt = st.empty()
                    
                    processed_global = 0
                    
                    # ğŸš€ æ‰§è¡Œå¾ªç¯
                    for batch_idx, batch_df in enumerate(batches):
                        batch_num = batch_idx + 1
                        batch_results = []
                        
                        global_txt.caption(f"æ­£åœ¨å¤„ç†åŒ… {batch_num}/{len(batches)} ...")
                        
                        for i, (orig_idx, row) in enumerate(batch_df.iterrows()):
                            try:
                                # æ•°æ®å‡†å¤‡
                                raw_name = str(row[col_name])
                                chain_name = str(row[col_chain]) if col_chain else ""
                                search_name = raw_name
                                if chain_name and chain_name not in raw_name: search_name = f"{chain_name} {raw_name}"
                                
                                row_with_meta = row.copy()
                                if col_addr: row_with_meta['åœ°å€åˆ—_raw'] = str(row[col_addr])

                                # æ£€ç´¢
                                indices, scope_desc = get_candidates_hierarchical(
                                    search_name, chain_name, df_master, 
                                    prov_groups, city_groups, dist_groups, chain_groups, 
                                    row, mapping
                                )
                                
                                base_res = row.to_dict()
                                
                                # ç»“æœåˆ¤æ–­
                                if not indices:
                                    base_res.update({"åŒ¹é…ESID": None, "åŒ¹é…æ ‡å‡†å": None, "æœºæ„ç±»å‹": None, "ç½®ä¿¡åº¦": "Low", "åŒ¹é…æ–¹å¼": "æ— ç»“æœ", "ç†ç”±": f"èŒƒå›´[{scope_desc}]å†…æ— å€™é€‰"})
                                    stats['no_match'] += 1
                                else:
                                    candidates = df_master.loc[indices].copy()
                                    if candidates.empty:
                                        base_res.update({"åŒ¹é…ESID": None, "åŒ¹é…æ ‡å‡†å": None, "æœºæ„ç±»å‹": None, "ç½®ä¿¡åº¦": "Low", "åŒ¹é…æ–¹å¼": "æ— ç»“æœ", "ç†ç”±": "ç´¢å¼•å¼‚å¸¸"})
                                        stats['no_match'] += 1
                                    else:
                                        # AI åŒ¹é…
                                        ai_res = ai_match_row_v3(client, row_with_meta, search_name, chain_name, scope_desc, candidates)
                                        if isinstance(ai_res, list): ai_res = ai_res[0] if ai_res else {}
                                        
                                        conf = ai_res.get("confidence", "Low")
                                        base_res.update({
                                            "åŒ¹é…ESID": ai_res.get("match_esid"),
                                            "åŒ¹é…æ ‡å‡†å": ai_res.get("match_name"),
                                            "æœºæ„ç±»å‹": ai_res.get("match_type"),
                                            "ç½®ä¿¡åº¦": conf,
                                            "åŒ¹é…æ–¹å¼": f"æ¨¡å‹ ({scope_desc})",
                                            "ç†ç”±": ai_res.get("reason")
                                        })
                                        
                                        if conf == "High": stats['high'] += 1
                                        elif conf == "Mid": stats['mid'] += 1
                                        else: stats['low'] += 1
                                        
                                        time.sleep(1.5) # å†·å´
                                
                                batch_results.append(base_res)
                                
                                # æ›´æ–°è¿›åº¦
                                processed_global += 1
                                batch_prog.progress((i + 1) / len(batch_df))
                                batch_txt.caption(f"è¿›åº¦: {i+1}/{len(batch_df)}")
                                global_prog.progress(processed_global / count_rem)
                                
                            except Exception as e:
                                pass
                        
                        # --- æ‰¹æ¬¡å­˜æ¡£ ---
                        if batch_results:
                            df_batch = pd.DataFrame(batch_results)
                            final_accumulated = pd.concat([final_accumulated, df_batch], ignore_index=True)
                            st.session_state.final_result_df = final_accumulated
                            st.session_state.match_stats = stats
                            st.toast(f"âœ… ä»»åŠ¡åŒ… {batch_num} å®Œæˆå¹¶å­˜æ¡£", icon="ğŸ’¾")
                            del df_batch
                            gc.collect()

                    st.success("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæˆï¼")
                    st.rerun()
            
            else:
                # åªæœ‰å…¨å­—åŒ¹é…
                if st.button("âœ¨ ç›´æ¥ç”Ÿæˆç»“æœ", type="primary"):
                    st.session_state.final_result_df = st.session_state.df_exact
                    st.session_state.match_stats = {'exact': count_exact, 'high': 0, 'mid': 0, 'low': 0, 'no_match': 0}
                    st.rerun()

# --- 4. ç»“æœå±•ç¤º ---
if st.session_state.final_result_df is not None:
    s = st.session_state.match_stats
    total = len(st.session_state.final_result_df)
    if total == 0: total = 1
    
    st.markdown("### ğŸ“Š åŒ¹é…ç»Ÿè®¡æŠ¥å‘Š")
    
    exact_val = s.get('exact', 0)
    exact_pct = exact_val / total
    
    model_done = s.get('high', 0) + s.get('mid', 0) + s.get('low', 0)
    model_pct = model_done / total
    model_denom = model_done if model_done > 0 else 1
    
    high_pct = s.get('high', 0) / model_denom
    mid_pct = s.get('mid', 0) / model_denom
    low_pct = s.get('low', 0) / model_denom
    
    c1, c2, c3, c4, c5 = st.columns(5)
    with c1: st.metric("ğŸ¯ å…¨å­—åŒ¹é…", f"{exact_val}", f"{exact_pct:.1%}")
    with c2: st.metric("ğŸ¤– æ¨¡å‹æ€»è®¡", f"{model_done}", f"{model_pct:.1%}")
    with c3: st.metric("ğŸ”¥ High", f"{s.get('high', 0)}", f"{high_pct:.1%} (of Model)")
    with c4: st.metric("âš–ï¸ Mid", f"{s.get('mid', 0)}", f"{mid_pct:.1%} (of Model)")
    with c5: st.metric("âš ï¸ Low", f"{s.get('low', 0)}", f"{low_pct:.1%} (of Model)")

    st.divider()
    
    def color_row(row):
        conf = row.get('ç½®ä¿¡åº¦')
        if conf == 'High': return ['background-color: #dcfce7'] * len(row)
        if conf == 'Mid': return ['background-color: #fef9c3'] * len(row)
        if conf == 'Low': return ['background-color: #fee2e2'] * len(row)
        return [''] * len(row)

    df_show = st.session_state.final_result_df
    st.dataframe(df_show.style.apply(color_row, axis=1), use_container_width=True)
    
    csv = df_show.to_csv(index=False).encode('utf-8-sig')
    st.download_button("ğŸ“¥ ä¸‹è½½å®Œæ•´ç»“æœ", csv, "linkmed_batch_result.csv", "text/csv", type="primary")




