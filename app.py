import streamlit as st
import pandas as pd
import json
import time
import os
import gc
import math
from google import genai
from google.genai import types
from rapidfuzz import process, fuzz 

# ================= 1. é…ç½®ä¸åˆå§‹åŒ– =================

st.set_page_config(page_title="LinkMed Matcher Pro (Clean)", layout="wide", page_icon="ğŸ§¬")

try:
    FIXED_API_KEY = st.secrets["GENAI_API_KEY"]
except:
    FIXED_API_KEY = "" 

LOCAL_MASTER_FILE = "MDM_retail.xlsx"

if 'uploader_key' not in st.session_state:
    st.session_state.uploader_key = str(time.time())
if 'final_result_df' not in st.session_state:
    st.session_state.final_result_df = None
if 'match_stats' not in st.session_state:
    st.session_state.match_stats = {}
if 'batch_progress' not in st.session_state:
    st.session_state.batch_progress = [] 

# ================= 2. æ ¸å¿ƒå·¥å…·å‡½æ•° =================

def reset_app():
    st.session_state.final_result_df = None
    st.session_state.match_stats = {}
    st.session_state.batch_progress = []
    st.session_state.uploader_key = str(time.time())
    st.rerun()

@st.cache_resource
def get_client():
    if not FIXED_API_KEY: return None
    return genai.Client(api_key=FIXED_API_KEY, http_options={'api_version': 'v1beta'})

def safe_generate(client, prompt, response_schema=None, retries=3):
    if client is None: return {"error": "API Key æœªé…ç½®"}
    wait_time = 2 
    for attempt in range(retries):
        try:
            config = types.GenerateContentConfig(
                response_mime_type="application/json",
                response_schema=response_schema
            )
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                contents=prompt,
                config=config
            )
            try:
                return json.loads(response.text)
            except json.JSONDecodeError:
                return {"error": "JSONè§£æå¤±è´¥", "raw": response.text}
        except Exception as e:
            if "429" in str(e) or "503" in str(e):
                if attempt < retries - 1:
                    time.sleep(wait_time * (2 ** attempt))
                    continue
            return {"error": str(e)}
    return {"error": "Max retries reached"}

@st.cache_resource(show_spinner=False)
def load_master_data():
    """
    æ ‡å‡†åŠ è½½æ¨¡å¼ï¼šæ¯æ¬¡ç›´æ¥è¯»å– Excel/CSVï¼Œä¸ä½¿ç”¨ Pickle ç¼“å­˜
    ç¡®ä¿æ•°æ® 100% å‡†ç¡®ï¼Œæ— ç¼“å­˜å¹²æ‰°
    """
    if os.path.exists(LOCAL_MASTER_FILE):
        try:
            gc.collect()
            # æ ¹æ®åç¼€è¯»å–
            if LOCAL_MASTER_FILE.endswith('.xlsx'):
                df = pd.read_excel(LOCAL_MASTER_FILE, engine='openpyxl')
            else:
                df = pd.read_csv(LOCAL_MASTER_FILE)
            
            # åŸºç¡€æ¸…æ´—
            df = df.reset_index(drop=True)
            target_cols = ['æ ‡å‡†åç§°', 'çœ', 'å¸‚', 'åŒº', 'æœºæ„ç±»å‹', 'åœ°å€', 'è¿é”å“ç‰Œ']
            for col in target_cols:
                if col not in df.columns: df[col] = ''
                df[col] = df[col].astype(str).replace('nan', '').str.strip()
            
            # å»ºç«‹ç´¢å¼•
            prov_groups = df.groupby('çœ').groups
            city_groups = df.groupby('å¸‚').groups
            dist_groups = df.groupby('åŒº').groups
            
            chain_groups = {}
            mask = df['è¿é”å“ç‰Œ'].str.len() > 1
            if mask.any():
                chain_groups = df[mask].groupby('è¿é”å“ç‰Œ').groups
            
            return df, prov_groups, city_groups, dist_groups, chain_groups
        except Exception as e:
            st.error(f"è¯»å–ä¸»æ•°æ®é”™è¯¯: {e}")
            return pd.DataFrame(), {}, {}, {}, {}
    else:
        return pd.DataFrame(), {}, {}, {}, {}

def smart_map_columns(client, df_user):
    user_cols = df_user.columns.tolist()
    sample_data = df_user.head(3).to_markdown(index=False)
    prompt = f"""
    åˆ†æç”¨æˆ·æ•°æ®ï¼Œæ‰¾å‡ºä»¥ä¸‹å­—æ®µå¯¹åº”çš„åˆ—åã€‚
    ç”¨æˆ·åˆ—å: {user_cols}
    é¢„è§ˆ: {sample_data}
    ä»»åŠ¡ï¼šæ‰¾å‡ºä»¥ä¸‹åˆ—ï¼ˆæ— åˆ™nullï¼‰ï¼š
    1. name_col: è¯æˆ¿åç§°
    2. chain_col: è¿é”/å“ç‰Œåç§°
    3. prov_col: çœä»½
    4. city_col: åŸå¸‚
    5. dist_col: åŒº/å¿
    6. addr_col: è¯¦ç»†åœ°å€
    è¾“å‡º JSON: {{ "name_col": "...", "chain_col": "...", "prov_col": "...", "city_col": "...", "dist_col": "...", "addr_col": "..." }}
    """
    res = safe_generate(client, prompt)
    if isinstance(res, list): res = res[0] if res else {}
    return res

def get_candidates_hierarchical(search_name, chain_name, df_master, prov_groups, city_groups, dist_groups, chain_groups, user_row, mapping):
    try:
        u_prov = str(user_row[mapping['prov']]) if mapping['prov'] and pd.notna(user_row[mapping['prov']]) else ''
        u_city = str(user_row[mapping['city']]) if mapping['city'] and pd.notna(user_row[mapping['city']]) else ''
        u_dist = str(user_row[mapping['dist']]) if mapping['dist'] and pd.notna(user_row[mapping['dist']]) else ''
        
        target_indices = set()
        scope_desc = ""

        if u_dist and u_dist in dist_groups:
            dist_indices = set(dist_groups[u_dist])
            if u_city and u_city in city_groups:
                city_indices = set(city_groups[u_city])
                intersection = dist_indices.intersection(city_indices)
                target_indices = intersection if intersection else dist_indices
                scope_desc = f"ç²¾å‡†å®šä½: {u_city}{u_dist}"
            else:
                target_indices = dist_indices
                scope_desc = f"åŒºåŸŸå®šä½: {u_dist}"
        
        elif u_city and u_city in city_groups:
            target_indices = set(city_groups[u_city])
            scope_desc = f"åŸå¸‚å®šä½: {u_city}"
            
        elif u_prov and u_prov in prov_groups:
            target_indices = set(prov_groups[u_prov])
            scope_desc = f"çœä»½å®šä½: {u_prov}"
            
        else:
            target_indices = set(df_master.index)
            scope_desc = "å…¨å±€æœç´¢"

        force_chain_indices = set()
        if chain_name and chain_name in chain_groups:
            chain_indices = set(chain_groups[chain_name])
            force_chain_indices = chain_indices.intersection(target_indices)

        candidates_indices = set()
        candidates_indices.update(force_chain_indices) 
        
        if target_indices:
            search_pool_indices = list(target_indices)
            if len(search_pool_indices) > 5000 and len(force_chain_indices) > 0:
                 search_pool_indices = search_pool_indices[:2000] 

            current_scope_df = df_master.loc[search_pool_indices]
            choices = current_scope_df['æ ‡å‡†åç§°'].fillna('').astype(str).to_dict()
            
            results = process.extract(search_name, choices, limit=8, scorer=fuzz.WRatio)
            for r in results:
                candidates_indices.add(r[2])

        return list(candidates_indices), scope_desc
    
    except Exception as e:
        print(f"Retrieval Error: {e}")
        return [], "Error"

def ai_match_row_v3(client, user_row, search_name, chain_name, scope_desc, candidates_df):
    cols_to_keep = ['esid', 'æ ‡å‡†åç§°', 'æœºæ„ç±»å‹', 'çœ', 'å¸‚', 'åŒº', 'åœ°å€', 'è¿é”å“ç‰Œ']
    valid_cols = [c for c in cols_to_keep if c in candidates_df.columns]
    candidates_json = candidates_df[valid_cols].to_json(orient="records", force_ascii=False)
    
    prompt = f"""
    ã€è§’è‰²ã€‘ä¸»æ•°æ®åŒ¹é…ä¸“å®¶ã€‚
    ã€å¾…åŒ¹é…å®ä½“ã€‘
    - ç»„åˆåç§°: "{search_name}"
    - è¿é”å“ç‰Œ: "{chain_name}"
    - æ£€ç´¢èŒƒå›´: {scope_desc}
    - åŸå§‹åœ°å€: "{user_row.get('åœ°å€åˆ—_raw', '')}"
    
    ã€å€™é€‰ä¸»æ•°æ®ã€‘
    {candidates_json}
    
    ã€åŒ¹é…æ ‡å‡† - åˆ†çº§ç½®ä¿¡åº¦ã€‘:
    1. **High**: æ ¸å¿ƒåç§°ä¸€è‡´ä¸”åœ°å€/è¡Œæ”¿åŒºå»åˆã€‚
    2. **Mid**: æ˜¯åŒä¸€è¿é”ï¼Œä½†åˆ†åº—åæœ‰ç»†å¾®å·®å¼‚ï¼Œæˆ–åœ°å€ç¼ºå¤±ä½†åŒºåŸŸå†…ä»…æ­¤ä¸€å®¶ã€‚
    3. **Low**: åç§°ç›¸ä¼¼æ— æ³•ç¡®å®šï¼Œæˆ–åªæœ‰è¿é”åä¸€è‡´åˆ†åº—ä¸åŒã€‚
       
    ã€ç‰¹æ®Šè§„åˆ™ã€‘
    - **æ€»éƒ¨é™·é˜±**: é™¤éç”¨æˆ·æ‰¾æ€»éƒ¨ï¼Œå¦åˆ™ä¸è¦åŒ¹é…"æ€»å…¬å¸"ã€‚ä¼˜å…ˆåŒ¹é…é—¨åº—ã€‚
    
    ã€è¾“å‡º JSONã€‘:
    {{ "match_esid": "...", "match_name": "...", "match_type": "...", "confidence": "High/Mid/Low", "reason": "..." }}
    """
    return safe_generate(client, prompt)

# ================= 3. é¡µé¢ UI =================

st.markdown("""
    <style>
    .stApp {background-color: #F8F9FA;}
    .stat-card {background: #ffffff; padding: 15px; border-radius: 8px; border: 1px solid #e5e7eb; box-shadow: 0 1px 2px rgba(0,0,0,0.05);}
    .big-num {font-size: 24px; font-weight: bold; color: #1e40af;}
    .task-box {background-color: #f3f4f6; padding: 10px; border-radius: 5px; margin-bottom: 5px; border-left: 4px solid #3b82f6;}
    .prog-label {font-weight: bold; font-size: 14px; margin-bottom: 5px; display: block;}
    </style>
    <div style="font-size: 26px; font-weight: bold; color: #1E3A8A; margin-bottom: 20px;">
    ğŸ§¬ LinkMed Matcher (Clean Mode)
    </div>
""", unsafe_allow_html=True)

client = get_client()

# åŠ è½½æ•°æ®
df_master, prov_groups, city_groups, dist_groups, chain_groups = pd.DataFrame(), {}, {}, {}, {}
if os.path.exists(LOCAL_MASTER_FILE):
    with st.spinner(f"æ­£åœ¨åŠ è½½ä¸»æ•°æ® (å®æ—¶è¯»å–)..."):
        df_master, prov_groups, city_groups, dist_groups, chain_groups = load_master_data()
else:
    st.warning(f"âš ï¸ æ–‡ä»¶ç¼ºå¤±: `{LOCAL_MASTER_FILE}`")

# --- Sidebar ---
with st.sidebar:
    st.header("ğŸ—„ï¸ æ§åˆ¶å°")
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºé‡ç½®", type="secondary", use_container_width=True):
        reset_app()
    if not df_master.empty:
        st.success(f"ä¸»æ•°æ®: {len(df_master)} æ¡")

# --- ä¸»æµç¨‹ ---
if st.session_state.final_result_df is None:
    st.markdown("### ğŸ“‚ 1. ä¸Šä¼ æ•°æ®")
    uploaded_file = st.file_uploader("Excel/CSV", type=['xlsx', 'csv'], key=st.session_state.uploader_key)

    if uploaded_file and not df_master.empty:
        try:
            if uploaded_file.name.endswith('.csv'): df_user = pd.read_csv(uploaded_file)
            else: df_user = pd.read_excel(uploaded_file)
        except Exception as e:
            st.error(f"æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
            st.stop()
        
        # --- 2. å­—æ®µæ˜ å°„ ---
        st.markdown("### ğŸ¤– 2. å­—æ®µæ˜ å°„")
        if 'map_config' not in st.session_state or st.session_state.get('last_file') != uploaded_file.name:
            with st.spinner("AI æ­£åœ¨åˆ†æè¡¨å¤´..."):
                st.session_state.map_config = smart_map_columns(client, df_user)
                st.session_state.last_file = uploaded_file.name
        
        map_res = st.session_state.map_config
        cols = df_user.columns.tolist()
        
        c1, c2, c3 = st.columns(3)
        def get_idx(key): return cols.index(map_res.get(key)) if map_res.get(key) in cols else 0
        
        with c1:
            col_name = st.selectbox("ğŸ“ è¯æˆ¿åç§°", cols, index=get_idx('name_col'))
            col_chain = st.selectbox("ğŸ”— è¿é”åç§°", [None]+cols, index=cols.index(map_res['chain_col'])+1 if map_res.get('chain_col') in cols else 0)
        with c2:
            col_prov = st.selectbox("ğŸ—ºï¸ çœä»½", [None]+cols, index=cols.index(map_res['prov_col'])+1 if map_res.get('prov_col') in cols else 0)
            col_city = st.selectbox("ğŸ™ï¸ åŸå¸‚", [None]+cols, index=cols.index(map_res['city_col'])+1 if map_res.get('city_col') in cols else 0)
        with c3:
            col_dist = st.selectbox("ğŸ˜ï¸ åŒºå¿", [None]+cols, index=cols.index(map_res['dist_col'])+1 if map_res.get('dist_col') in cols else 0)
            col_addr = st.selectbox("ğŸ  è¯¦ç»†åœ°å€", [None]+cols, index=cols.index(map_res['addr_col'])+1 if map_res.get('addr_col') in cols else 0)

        mapping = {'prov': col_prov, 'city': col_city, 'dist': col_dist, 'addr': col_addr, 'chain': col_chain, 'name': col_name}

        # --- 3. é¢„å¤„ç†ä¸åˆ†åŒ… ---
        st.markdown("### âš¡ 3. é¢„å¤„ç†ä¸åˆ†åŒ…")
        
        # åˆ†ç»„é‡æ’
        sort_cols = []
        if col_prov: sort_cols.append(col_prov)
        if col_city: sort_cols.append(col_city)
        if col_dist: sort_cols.append(col_dist)
        
        if sort_cols:
            df_user_sorted = df_user.sort_values(by=sort_cols).reset_index(drop=True)
            st.caption(f"âœ… å·²æŒ‰åœ°ç†ä½ç½®é‡æ’æ•°æ®ï¼Œä¼˜åŒ–åŒ¹é…æ•ˆç‡ã€‚")
        else:
            df_user_sorted = df_user

        # å…¨å­—åŒ¹é…
        master_exact = df_master.drop_duplicates(subset=['æ ‡å‡†åç§°']).set_index('æ ‡å‡†åç§°').to_dict('index')
        exact_rows = []
        rem_indices = []
        
        with st.spinner("æ­£åœ¨è¿›è¡Œå…¨å­—åŒ¹é…é¢„ç­›é€‰..."):
            for idx, row in df_user_sorted.iterrows():
                raw_name = str(row[col_name]).strip()
                chain_name = str(row[col_chain]).strip() if col_chain and pd.notna(row[col_chain]) else ""
                search_name = raw_name
                if chain_name and chain_name not in raw_name: search_name = f"{chain_name} {raw_name}"
                
                if search_name in master_exact:
                    m = master_exact[search_name]
                    r = row.to_dict()
                    r.update({"åŒ¹é…ESID": m.get('esid'), "åŒ¹é…æ ‡å‡†å": search_name, "æœºæ„ç±»å‹": m.get('æœºæ„ç±»å‹'), "ç½®ä¿¡åº¦": "High", "åŒ¹é…æ–¹å¼": "å…¨å­—åŒ¹é…", "ç†ç”±": "ç²¾ç¡®å‘½ä¸­"})
                    exact_rows.append(r)
                else:
                    rem_indices.append(idx)
        
        df_exact = pd.DataFrame(exact_rows)
        df_rem = df_user_sorted.loc[rem_indices].copy().reset_index(drop=True)
        
        # æ‹†åŒ…é€»è¾‘
        BATCH_SIZE = 2000
        num_batches = 1
        batches = []
        
        if len(df_rem) > 0:
            num_batches = math.ceil(len(df_rem) / BATCH_SIZE)
            for i in range(num_batches):
                batches.append(df_rem.iloc[i*BATCH_SIZE : (i+1)*BATCH_SIZE])

        st.info(f"é¢„å¤„ç†æŠ¥å‘Š: è‡ªåŠ¨å‘½ä¸­ {len(df_exact)} è¡Œã€‚å‰©ä½™ {len(df_rem)} è¡Œå¾…æ¨¡å‹åŒ¹é…ã€‚")
        
        if len(df_rem) > 0:
            st.warning(f"ç”±äºæ•°æ®é‡è¾ƒå¤§ï¼Œå·²è‡ªåŠ¨æ‹†åˆ†ä¸º **{num_batches}** ä¸ªä»»åŠ¡åŒ…ã€‚")
            
            # æ˜¾ç¤ºåŒè¿›åº¦æ¡å ä½
            if st.button(f"ğŸš€ å¯åŠ¨ä»»åŠ¡é˜Ÿåˆ— ({len(df_rem)} è¡Œ)", type="primary"):
                
                final_accumulated = df_exact.copy() if not df_exact.empty else pd.DataFrame()
                stats = {'exact': len(df_exact), 'high': 0, 'mid': 0, 'low': 0, 'no_match': 0}
                
                st.write("") 
                col_g, col_b = st.columns(2)
                with col_g:
                    st.markdown('<span class="prog-label">ğŸŒ å…¨å±€æ€»è¿›åº¦</span>', unsafe_allow_html=True)
                    global_prog = st.progress(0)
                    global_txt = st.empty()
                
                with col_b:
                    st.markdown('<span class="prog-label">ğŸ“¦ å½“å‰ä»»åŠ¡åŒ…è¿›åº¦</span>', unsafe_allow_html=True)
                    batch_prog = st.progress(0)
                    batch_txt = st.empty()
                
                processed_global = 0
                
                for batch_idx, batch_df in enumerate(batches):
                    batch_num = batch_idx + 1
                    batch_results = []
                    
                    global_txt.caption(f"æ­£åœ¨å¤„ç†ç¬¬ {batch_num}/{num_batches} ä¸ªä»»åŠ¡åŒ…...")
                    
                    for i, (orig_idx, row) in enumerate(batch_df.iterrows()):
                        try:
                            # 1. ä¸šåŠ¡é€»è¾‘
                            raw_name = str(row[col_name]).strip()
                            chain_name = str(row[col_chain]).strip() if col_chain and pd.notna(row[col_chain]) else ""
                            search_name = raw_name
                            if chain_name and chain_name not in raw_name: search_name = f"{chain_name} {raw_name}"
                            
                            row_with_meta = row.copy()
                            if col_addr: row_with_meta['åœ°å€åˆ—_raw'] = str(row[col_addr])

                            indices, scope_desc = get_candidates_hierarchical(
                                search_name, chain_name, df_master, 
                                prov_groups, city_groups, dist_groups, chain_groups, 
                                row, mapping
                            )
                            
                            base_res = row.to_dict()
                            if not indices:
                                base_res.update({"åŒ¹é…ESID": None, "åŒ¹é…æ ‡å‡†å": None, "æœºæ„ç±»å‹": None, "ç½®ä¿¡åº¦": "Low", "åŒ¹é…æ–¹å¼": "æ— ç»“æœ", "ç†ç”±": f"èŒƒå›´[{scope_desc}]å†…æ— å€™é€‰"})
                                stats['no_match'] += 1
                            else:
                                try:
                                    candidates = df_master.loc[indices].copy()
                                except:
                                    candidates = pd.DataFrame()

                                if candidates.empty:
                                    base_res.update({"åŒ¹é…ESID": None, "åŒ¹é…æ ‡å‡†å": None, "æœºæ„ç±»å‹": None, "ç½®ä¿¡åº¦": "Low", "åŒ¹é…æ–¹å¼": "æ— ç»“æœ", "ç†ç”±": "ç´¢å¼•å¼‚å¸¸"})
                                    stats['no_match'] += 1
                                else:
                                    ai_res = ai_match_row_v3(client, row_with_meta, search_name, chain_name, scope_desc, candidates)
                                    if isinstance(ai_res, list): ai_res = ai_res[0] if ai_res else {}
                                    
                                    conf = ai_res.get("confidence", "Low")
                                    base_res.update({
                                        "åŒ¹é…ESID": ai_res.get("match_esid"),
                                        "åŒ¹é…æ ‡å‡†å": ai_res.get("match_name"),
                                        "æœºæ„ç±»å‹": ai_res.get("match_type"),
                                        "ç½®ä¿¡åº¦": conf,
                                        "åŒ¹é…æ–¹å¼": f"æ¨¡å‹ ({scope_desc})",
                                        "ç†ç”±": ai_res.get("reason")
                                    })
                                    
                                    if conf == "High": stats['high'] += 1
                                    elif conf == "Mid": stats['mid'] += 1
                                    else: stats['low'] += 1
                                    
                                    time.sleep(1.5) 
                            
                            batch_results.append(base_res)
                            
                            # 2. æ›´æ–°è¿›åº¦
                            processed_global += 1
                            batch_prog.progress((i + 1) / len(batch_df))
                            batch_txt.caption(f"å½“å‰åŒ…: {i+1} / {len(batch_df)} è¡Œ")
                            
                            global_prog.progress(processed_global / len(df_rem))
                            
                        except Exception as e:
                            st.warning(f"è¡Œé”™è¯¯: {e}")
                    
                    # æ‰¹æ¬¡å­˜æ¡£
                    if batch_results:
                        df_batch = pd.DataFrame(batch_results)
                        final_accumulated = pd.concat([final_accumulated, df_batch], ignore_index=True)
                        st.session_state.final_result_df = final_accumulated
                        st.session_state.match_stats = stats
                        st.toast(f"âœ… ä»»åŠ¡åŒ… {batch_num} å®Œæˆï¼å·²å­˜æ¡£ã€‚", icon="ğŸ’¾")

                st.success("ğŸ‰ æ‰€æœ‰ä»»åŠ¡åŒ…å¤„ç†å®Œæˆï¼")
                st.rerun()
        
        else:
            if st.button("âœ¨ ç›´æ¥ç”Ÿæˆç»“æœ", type="primary"):
                st.session_state.final_result_df = df_exact
                st.session_state.match_stats = {'exact': len(df_exact), 'high': 0, 'mid': 0, 'low': 0, 'no_match': 0}
                st.rerun()

# --- 4. ç»“æœå±•ç¤º ---
if st.session_state.final_result_df is not None:
    s = st.session_state.match_stats
    total = len(st.session_state.final_result_df)
    if total == 0: total = 1
    
    st.markdown("### ğŸ“Š åŒ¹é…ç»Ÿè®¡æŠ¥å‘Š")
    
    exact_val = s.get('exact', 0)
    exact_pct = exact_val / total
    
    model_done = s.get('high', 0) + s.get('mid', 0) + s.get('low', 0)
    model_pct = model_done / total
    model_denom = model_done if model_done > 0 else 1
    
    high_pct = s.get('high', 0) / model_denom
    mid_pct = s.get('mid', 0) / model_denom
    low_pct = s.get('low', 0) / model_denom
    
    c1, c2, c3, c4, c5 = st.columns(5)
    with c1: st.metric("ğŸ¯ å…¨å­—åŒ¹é…", f"{exact_val}", f"{exact_pct:.1%}")
    with c2: st.metric("ğŸ¤– æ¨¡å‹æ€»è®¡", f"{model_done}", f"{model_pct:.1%}")
    with c3: st.metric("ğŸ”¥ High", f"{s.get('high', 0)}", f"{high_pct:.1%} (of Model)")
    with c4: st.metric("âš–ï¸ Mid", f"{s.get('mid', 0)}", f"{mid_pct:.1%} (of Model)")
    with c5: st.metric("âš ï¸ Low", f"{s.get('low', 0)}", f"{low_pct:.1%} (of Model)")

    st.divider()
    
    def color_row(row):
        conf = row.get('ç½®ä¿¡åº¦')
        if conf == 'High': return ['background-color: #dcfce7'] * len(row)
        if conf == 'Mid': return ['background-color: #fef9c3'] * len(row)
        if conf == 'Low': return ['background-color: #fee2e2'] * len(row)
        return [''] * len(row)

    df_show = st.session_state.final_result_df
    st.dataframe(df_show.style.apply(color_row, axis=1), use_container_width=True)
    
    csv = df_show.to_csv(index=False).encode('utf-8-sig')
    st.download_button("ğŸ“¥ ä¸‹è½½å®Œæ•´ç»“æœ", csv, "linkmed_batch_result.csv", "text/csv", type="primary")
