import streamlit as st
import pandas as pd
import json
import time
import os
import gc
import math
from google import genai
from google.genai import types
from rapidfuzz import process, fuzz 

# ================= 1. é…ç½®ä¸åˆå§‹åŒ– =================

st.set_page_config(page_title="LinkMed Matcher Pro (Batch)", layout="wide", page_icon="ğŸ§¬")

try:
    FIXED_API_KEY = st.secrets["GENAI_API_KEY"]
except:
    FIXED_API_KEY = "" 

LOCAL_MASTER_FILE = "MDM_retail.xlsx"

if 'uploader_key' not in st.session_state:
    st.session_state.uploader_key = str(time.time())
if 'final_result_df' not in st.session_state:
    st.session_state.final_result_df = None
if 'match_stats' not in st.session_state:
    st.session_state.match_stats = {}
if 'batch_progress' not in st.session_state:
    st.session_state.batch_progress = [] # å­˜å‚¨å·²å®Œæˆçš„æ‰¹æ¬¡ä¿¡æ¯

# ================= 2. æ ¸å¿ƒå·¥å…·å‡½æ•° =================

def reset_app():
    st.session_state.final_result_df = None
    st.session_state.match_stats = {}
    st.session_state.batch_progress = []
    st.session_state.uploader_key = str(time.time())
    st.rerun()

@st.cache_resource
def get_client():
    if not FIXED_API_KEY: return None
    return genai.Client(api_key=FIXED_API_KEY, http_options={'api_version': 'v1beta'})

def safe_generate(client, prompt, response_schema=None, retries=3):
    if client is None: return {"error": "API Key æœªé…ç½®"}
    wait_time = 2 
    for attempt in range(retries):
        try:
            config = types.GenerateContentConfig(
                response_mime_type="application/json",
                response_schema=response_schema
            )
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                contents=prompt,
                config=config
            )
            try:
                return json.loads(response.text)
            except json.JSONDecodeError:
                return {"error": "JSONè§£æå¤±è´¥", "raw": response.text}
        except Exception as e:
            if "429" in str(e) or "503" in str(e):
                if attempt < retries - 1:
                    time.sleep(wait_time * (2 ** attempt))
                    continue
            return {"error": str(e)}
    return {"error": "Max retries reached"}

@st.cache_resource(show_spinner=False)
def load_master_data():
    """åŠ è½½å¹¶å»ºç«‹ä¸¥æ ¼çš„åœ°ç†åˆ†å±‚ç´¢å¼•"""
    if os.path.exists(LOCAL_MASTER_FILE):
        try:
            gc.collect()
            if LOCAL_MASTER_FILE.endswith('.xlsx'):
                df = pd.read_excel(LOCAL_MASTER_FILE, engine='openpyxl')
            else:
                df = pd.read_csv(LOCAL_MASTER_FILE)
            
            df = df.reset_index(drop=True)
            target_cols = ['æ ‡å‡†åç§°', 'çœ', 'å¸‚', 'åŒº', 'æœºæ„ç±»å‹', 'åœ°å€', 'è¿é”å“ç‰Œ']
            for col in target_cols:
                if col not in df.columns: df[col] = ''
                df[col] = df[col].astype(str).replace('nan', '').str.strip()
                
            prov_groups = df.groupby('çœ').groups
            city_groups = df.groupby('å¸‚').groups
            dist_groups = df.groupby('åŒº').groups
            
            chain_groups = {}
            mask = df['è¿é”å“ç‰Œ'].str.len() > 1
            if mask.any():
                chain_groups = df[mask].groupby('è¿é”å“ç‰Œ').groups
            
            return df, prov_groups, city_groups, dist_groups, chain_groups
        except Exception as e:
            st.error(f"è¯»å–ä¸»æ•°æ®é”™è¯¯: {e}")
            return pd.DataFrame(), {}, {}, {}, {}
    else:
        return pd.DataFrame(), {}, {}, {}, {}

def smart_map_columns(client, df_user):
    user_cols = df_user.columns.tolist()
    sample_data = df_user.head(3).to_markdown(index=False)
    prompt = f"""
    åˆ†æç”¨æˆ·æ•°æ®ï¼Œæ‰¾å‡ºä»¥ä¸‹å­—æ®µå¯¹åº”çš„åˆ—åã€‚
    ç”¨æˆ·åˆ—å: {user_cols}
    é¢„è§ˆ: {sample_data}
    ä»»åŠ¡ï¼šæ‰¾å‡ºä»¥ä¸‹åˆ—ï¼ˆæ— åˆ™nullï¼‰ï¼š
    1. name_col: è¯æˆ¿åç§°
    2. chain_col: è¿é”/å“ç‰Œåç§°
    3. prov_col: çœä»½
    4. city_col: åŸå¸‚
    5. dist_col: åŒº/å¿
    6. addr_col: è¯¦ç»†åœ°å€
    è¾“å‡º JSON: {{ "name_col": "...", "chain_col": "...", "prov_col": "...", "city_col": "...", "dist_col": "...", "addr_col": "..." }}
    """
    res = safe_generate(client, prompt)
    if isinstance(res, list): res = res[0] if res else {}
    return res

def get_candidates_hierarchical(search_name, chain_name, df_master, prov_groups, city_groups, dist_groups, chain_groups, user_row, mapping):
    try:
        u_prov = str(user_row[mapping['prov']]) if mapping['prov'] and pd.notna(user_row[mapping['prov']]) else ''
        u_city = str(user_row[mapping['city']]) if mapping['city'] and pd.notna(user_row[mapping['city']]) else ''
        u_dist = str(user_row[mapping['dist']]) if mapping['dist'] and pd.notna(user_row[mapping['dist']]) else ''
        
        target_indices = set()
        scope_desc = ""

        if u_dist and u_dist in dist_groups:
            dist_indices = set(dist_groups[u_dist])
            if u_city and u_city in city_groups:
                city_indices = set(city_groups[u_city])
                intersection = dist_indices.intersection(city_indices)
                if intersection:
                    target_indices = intersection
                    scope_desc = f"ç²¾å‡†å®šä½: {u_city}{u_dist}"
                else:
                    target_indices = dist_indices
                    scope_desc = f"åŒºåŸŸå®šä½: {u_dist}"
            else:
                target_indices = dist_indices
                scope_desc = f"åŒºåŸŸå®šä½: {u_dist}"
        
        elif u_city and u_city in city_groups:
            target_indices = set(city_groups[u_city])
            scope_desc = f"åŸå¸‚å®šä½: {u_city}"
            
        elif u_prov and u_prov in prov_groups:
            target_indices = set(prov_groups[u_prov])
            scope_desc = f"çœä»½å®šä½: {u_prov}"
            
        else:
            target_indices = set(df_master.index)
            scope_desc = "å…¨å±€æœç´¢"

        force_chain_indices = set()
        if chain_name and chain_name in chain_groups:
            chain_indices = set(chain_groups[chain_name])
            force_chain_indices = chain_indices.intersection(target_indices)

        candidates_indices = set()
        candidates_indices.update(force_chain_indices) 
        
        if target_indices:
            search_pool_indices = list(target_indices)
            if len(search_pool_indices) > 5000 and len(force_chain_indices) > 0:
                 search_pool_indices = search_pool_indices[:2000] 

            current_scope_df = df_master.loc[search_pool_indices]
            choices = current_scope_df['æ ‡å‡†åç§°'].fillna('').astype(str).to_dict()
            
            results = process.extract(search_name, choices, limit=8, scorer=fuzz.WRatio)
            for r in results:
                candidates_indices.add(r[2])

        return list(candidates_indices), scope_desc
    
    except Exception as e:
        print(f"Retrieval Error: {e}")
        return [], "Error"

def ai_match_row_v3(client, user_row, search_name, chain_name, scope_desc, candidates_df):
    cols_to_keep = ['esid', 'æ ‡å‡†åç§°', 'æœºæ„ç±»å‹', 'çœ', 'å¸‚', 'åŒº', 'åœ°å€', 'è¿é”å“ç‰Œ']
    valid_cols = [c for c in cols_to_keep if c in candidates_df.columns]
    candidates_json = candidates_df[valid_cols].to_json(orient="records", force_ascii=False)
    
    prompt = f"""
    ã€è§’è‰²ã€‘ä¸»æ•°æ®åŒ¹é…ä¸“å®¶ã€‚
    ã€å¾…åŒ¹é…å®ä½“ã€‘
    - ç»„åˆåç§°: "{search_name}"
    - è¿é”å“ç‰Œ: "{chain_name}"
    - æ£€ç´¢èŒƒå›´: {scope_desc}
    - åŸå§‹åœ°å€: "{user_row.get('åœ°å€åˆ—_raw', '')}"
    
    ã€å€™é€‰ä¸»æ•°æ®ã€‘
    {candidates_json}
    
    ã€åŒ¹é…æ ‡å‡† - åˆ†çº§ç½®ä¿¡åº¦ã€‘:
    1. **High**: æ ¸å¿ƒåç§°ä¸€è‡´ä¸”åœ°å€/è¡Œæ”¿åŒºå»åˆã€‚
    2. **Mid**: æ˜¯åŒä¸€è¿é”ï¼Œä½†åˆ†åº—åæœ‰ç»†å¾®å·®å¼‚ï¼Œæˆ–åœ°å€ç¼ºå¤±ä½†åŒºåŸŸå†…ä»…æ­¤ä¸€å®¶ã€‚
    3. **Low**: åç§°ç›¸ä¼¼æ— æ³•ç¡®å®šï¼Œæˆ–åªæœ‰è¿é”åä¸€è‡´åˆ†åº—ä¸åŒã€‚
       
    ã€ç‰¹æ®Šè§„åˆ™ã€‘
    - **æ€»éƒ¨é™·é˜±**: é™¤éç”¨æˆ·æ‰¾æ€»éƒ¨ï¼Œå¦åˆ™ä¸è¦åŒ¹é…"æ€»å…¬å¸"ã€‚ä¼˜å…ˆåŒ¹é…é—¨åº—ã€‚
    
    ã€è¾“å‡º JSONã€‘:
    {{ "match_esid": "...", "match_name": "...", "match_type": "...", "confidence": "High/Mid/Low", "reason": "..." }}
    """
    return safe_generate(client, prompt)

# ================= 3. é¡µé¢ UI =================

st.markdown("""
    <style>
    .stApp {background-color: #F8F9FA;}
    .stat-card {background: #ffffff; padding: 15px; border-radius: 8px; border: 1px solid #e5e7eb; box-shadow: 0 1px 2px rgba(0,0,0,0.05);}
    .big-num {font-size: 24px; font-weight: bold; color: #1e40af;}
    .sub-text {font-size: 14px; color: #6b7280;}
    .task-box {background-color: #f3f4f6; padding: 10px; border-radius: 5px; margin-bottom: 5px; border-left: 4px solid #3b82f6;}
    </style>
    <div style="font-size: 26px; font-weight: bold; color: #1E3A8A; margin-bottom: 20px;">
    ğŸ§¬ LinkMed Matcher (Batch Processing)
    </div>
""", unsafe_allow_html=True)

client = get_client()

# åŠ è½½æ•°æ®
df_master, prov_groups, city_groups, dist_groups, chain_groups = pd.DataFrame(), {}, {}, {}, {}
if os.path.exists(LOCAL_MASTER_FILE):
    with st.spinner(f"æ­£åœ¨æ„å»ºåˆ†å±‚åœ°ç†ç´¢å¼•..."):
        df_master, prov_groups, city_groups, dist_groups, chain_groups = load_master_data()
else:
    st.warning(f"âš ï¸ æ–‡ä»¶ç¼ºå¤±: `{LOCAL_MASTER_FILE}`")

# --- Sidebar ---
with st.sidebar:
    st.header("ğŸ—„ï¸ æ§åˆ¶å°")
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºé‡ç½®", type="secondary", use_container_width=True):
        reset_app()
    if not df_master.empty:
        st.success(f"ä¸»æ•°æ®: {len(df_master)} æ¡")

# --- ä¸»æµç¨‹ ---
if st.session_state.final_result_df is None:
    st.markdown("### ğŸ“‚ 1. ä¸Šä¼ æ•°æ®")
    uploaded_file = st.file_uploader("Excel/CSV", type=['xlsx', 'csv'], key=st.session_state.uploader_key)

    if uploaded_file and not df_master.empty:
        try:
            if uploaded_file.name.endswith('.csv'): df_user = pd.read_csv(uploaded_file)
            else: df_user = pd.read_excel(uploaded_file)
        except Exception as e:
            st.error(f"æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
            st.stop()
        
        # --- 2. å­—æ®µæ˜ å°„ ---
        st.markdown("### ğŸ¤– 2. å­—æ®µæ˜ å°„")
        if 'map_config' not in st.session_state or st.session_state.get('last_file') != uploaded_file.name:
            with st.spinner("AI æ­£åœ¨åˆ†æè¡¨å¤´..."):
                st.session_state.map_config = smart_map_columns(client, df_user)
                st.session_state.last_file = uploaded_file.name
        
        map_res = st.session_state.map_config
        cols = df_user.columns.tolist()
        
        c1, c2, c3 = st.columns(3)
        def get_idx(key): return cols.index(map_res.get(key)) if map_res.get(key) in cols else 0
        
        with c1:
            col_name = st.selectbox("ğŸ“ è¯æˆ¿åç§°", cols, index=get_idx('name_col'))
            col_chain = st.selectbox("ğŸ”— è¿é”åç§°", [None]+cols, index=cols.index(map_res['chain_col'])+1 if map_res.get('chain_col') in cols else 0)
        with c2:
            col_prov = st.selectbox("ğŸ—ºï¸ çœä»½", [None]+cols, index=cols.index(map_res['prov_col'])+1 if map_res.get('prov_col') in cols else 0)
            col_city = st.selectbox("ğŸ™ï¸ åŸå¸‚", [None]+cols, index=cols.index(map_res['city_col'])+1 if map_res.get('city_col') in cols else 0)
        with c3:
            col_dist = st.selectbox("ğŸ˜ï¸ åŒºå¿", [None]+cols, index=cols.index(map_res['dist_col'])+1 if map_res.get('dist_col') in cols else 0)
            col_addr = st.selectbox("ğŸ  è¯¦ç»†åœ°å€", [None]+cols, index=cols.index(map_res['addr_col'])+1 if map_res.get('addr_col') in cols else 0)

        mapping = {'prov': col_prov, 'city': col_city, 'dist': col_dist, 'addr': col_addr, 'chain': col_chain, 'name': col_name}

        # --- 3. é¢„å¤„ç†ä¸é‡æ’ ---
        st.markdown("### âš¡ 3. é¢„å¤„ç†ä¸åˆ†åŒ…")
        
        # ğŸŒŸ ç¬¬ä¸€æ­¥ï¼šåˆ†ç»„é‡æ’ (Clustering)
        # å°†æ•°æ®æŒ‰çœå¸‚åŒºæ’åºï¼Œä¿è¯åŒä¸€ä¸ªåœ°åŒºçš„åº—åœ¨ä¸€èµ·ï¼Œåˆ©äºåˆ†åŒ…
        sort_cols = []
        if col_prov: sort_cols.append(col_prov)
        if col_city: sort_cols.append(col_city)
        if col_dist: sort_cols.append(col_dist)
        
        if sort_cols:
            df_user_sorted = df_user.sort_values(by=sort_cols).reset_index(drop=True)
            st.caption(f"âœ… å·²æŒ‰åœ°ç†ä½ç½®é‡æ’æ•°æ®ï¼Œä¼˜åŒ–åŒ¹é…æ•ˆç‡ã€‚")
        else:
            df_user_sorted = df_user

        # ğŸŒŸ ç¬¬äºŒæ­¥ï¼šå…¨å­—åŒ¹é…è¿‡æ»¤
        master_exact = df_master.drop_duplicates(subset=['æ ‡å‡†åç§°']).set_index('æ ‡å‡†åç§°').to_dict('index')
        
        exact_rows = []
        rem_indices = []
        
        with st.spinner("æ­£åœ¨è¿›è¡Œå…¨å­—åŒ¹é…é¢„ç­›é€‰..."):
            for idx, row in df_user_sorted.iterrows():
                raw_name = str(row[col_name]).strip()
                chain_name = str(row[col_chain]).strip() if col_chain and pd.notna(row[col_chain]) else ""
                search_name = raw_name
                if chain_name and chain_name not in raw_name: search_name = f"{chain_name} {raw_name}"
                
                if search_name in master_exact:
                    m = master_exact[search_name]
                    r = row.to_dict()
                    r.update({"åŒ¹é…ESID": m.get('esid'), "åŒ¹é…æ ‡å‡†å": search_name, "æœºæ„ç±»å‹": m.get('æœºæ„ç±»å‹'), "ç½®ä¿¡åº¦": "High", "åŒ¹é…æ–¹å¼": "å…¨å­—åŒ¹é…", "ç†ç”±": "ç²¾ç¡®å‘½ä¸­"})
                    exact_rows.append(r)
                else:
                    rem_indices.append(idx)
        
        df_exact = pd.DataFrame(exact_rows)
        df_rem = df_user_sorted.loc[rem_indices].copy().reset_index(drop=True)
        
        # ğŸŒŸ ç¬¬ä¸‰æ­¥ï¼šæ™ºèƒ½æ‹†åŒ… (Batch Splitting)
        # å¦‚æœå‰©ä½™æ•°æ® > 2000ï¼Œåˆ™æ‹†åˆ†
        BATCH_SIZE = 2000
        num_batches = 1
        batches = []
        
        if len(df_rem) > 0:
            num_batches = math.ceil(len(df_rem) / BATCH_SIZE)
            for i in range(num_batches):
                start_i = i * BATCH_SIZE
                end_i = min((i + 1) * BATCH_SIZE, len(df_rem))
                batch_df = df_rem.iloc[start_i:end_i]
                batches.append(batch_df)

        st.info(f"é¢„å¤„ç†æŠ¥å‘Š: è‡ªåŠ¨å‘½ä¸­ {len(df_exact)} è¡Œã€‚å‰©ä½™ {len(df_rem)} è¡Œå¾…æ¨¡å‹åŒ¹é…ã€‚")
        
        if len(df_rem) > 0:
            st.warning(f"ç”±äºæ•°æ®é‡è¾ƒå¤§ï¼Œå·²è‡ªåŠ¨æ‹†åˆ†ä¸º **{num_batches}** ä¸ªä»»åŠ¡åŒ…ï¼Œå°†ä¾æ¬¡æ‰§è¡Œå¹¶è‡ªåŠ¨åˆå¹¶ç»“æœã€‚")
            with st.expander("æŸ¥çœ‹ä»»åŠ¡é˜Ÿåˆ—", expanded=True):
                for i, b in enumerate(batches):
                    # å°è¯•è¯»å–è¯¥æ‰¹æ¬¡çš„ç¬¬ä¸€ä¸ªåœ°ç†ä½ç½®ä½œä¸ºæ ‡è¯†
                    loc_tag = "æœªçŸ¥åŒºåŸŸ"
                    if len(b) > 0:
                        first_row = b.iloc[0]
                        p = str(first_row[col_prov]) if col_prov and pd.notna(first_row[col_prov]) else ""
                        c = str(first_row[col_city]) if col_city and pd.notna(first_row[col_city]) else ""
                        loc_tag = f"{p} {c}".strip() or "æ··åˆåŒºåŸŸ"
                    st.markdown(f"<div class='task-box'>ğŸ“¦ <b>ä»»åŠ¡åŒ… {i+1}</b>: {len(b)} è¡Œ (èµ·å§‹ä½ç½®: {loc_tag})</div>", unsafe_allow_html=True)

            if st.button(f"ğŸš€ å¯åŠ¨ä»»åŠ¡é˜Ÿåˆ— ({len(df_rem)} è¡Œ)", type="primary"):
                
                # åˆå§‹åŒ–ç»“æœå®¹å™¨ (åŒ…å«å·²å…¨å­—åŒ¹é…çš„)
                if df_exact.empty:
                    final_accumulated = pd.DataFrame()
                else:
                    final_accumulated = df_exact.copy()
                
                stats = {'exact': len(df_exact), 'high': 0, 'mid': 0, 'low': 0, 'no_match': 0}
                
                # è¿›åº¦æ¡ (é’ˆå¯¹æ‰€æœ‰æ¨¡å‹ä»»åŠ¡)
                total_rem_rows = len(df_rem)
                global_prog = st.progress(0)
                status_text = st.empty()
                
                processed_count = 0
                
                # --- å¾ªç¯æ‰§è¡Œæ‰¹æ¬¡ ---
                for batch_idx, batch_df in enumerate(batches):
                    
                    status_text.markdown(f"### ğŸ”„ æ­£åœ¨å¤„ç†ä»»åŠ¡åŒ… {batch_idx + 1} / {num_batches} ...")
                    batch_results = []
                    
                    for i, (orig_idx, row) in enumerate(batch_df.iterrows()):
                        try:
                            # å‡†å¤‡æ•°æ®
                            raw_name = str(row[col_name]).strip()
                            chain_name = str(row[col_chain]).strip() if col_chain and pd.notna(row[col_chain]) else ""
                            search_name = raw_name
                            if chain_name and chain_name not in raw_name: search_name = f"{chain_name} {raw_name}"
                            
                            row_with_meta = row.copy()
                            if col_addr: row_with_meta['åœ°å€åˆ—_raw'] = str(row[col_addr])

                            # è°ƒç”¨åˆ†å±‚æ£€ç´¢
                            indices, scope_desc = get_candidates_hierarchical(
                                search_name, chain_name, df_master, 
                                prov_groups, city_groups, dist_groups, chain_groups, 
                                row, mapping
                            )
                            
                            base_res = row.to_dict()
                            if not indices:
                                base_res.update({"åŒ¹é…ESID": None, "åŒ¹é…æ ‡å‡†å": None, "æœºæ„ç±»å‹": None, "ç½®ä¿¡åº¦": "Low", "åŒ¹é…æ–¹å¼": "æ— ç»“æœ", "ç†ç”±": f"èŒƒå›´[{scope_desc}]å†…æ— å€™é€‰"})
                                stats['no_match'] += 1
                            else:
                                try:
                                    candidates = df_master.loc[indices].copy()
                                except:
                                    candidates = pd.DataFrame()

                                if candidates.empty:
                                    base_res.update({"åŒ¹é…ESID": None, "åŒ¹é…æ ‡å‡†å": None, "æœºæ„ç±»å‹": None, "ç½®ä¿¡åº¦": "Low", "åŒ¹é…æ–¹å¼": "æ— ç»“æœ", "ç†ç”±": "ç´¢å¼•å¼‚å¸¸"})
                                    stats['no_match'] += 1
                                else:
                                    # è°ƒç”¨ AI
                                    ai_res = ai_match_row_v3(client, row_with_meta, search_name, chain_name, scope_desc, candidates)
                                    if isinstance(ai_res, list): ai_res = ai_res[0] if ai_res else {}
                                    
                                    conf = ai_res.get("confidence", "Low")
                                    base_res.update({
                                        "åŒ¹é…ESID": ai_res.get("match_esid"),
                                        "åŒ¹é…æ ‡å‡†å": ai_res.get("match_name"),
                                        "æœºæ„ç±»å‹": ai_res.get("match_type"),
                                        "ç½®ä¿¡åº¦": conf,
                                        "åŒ¹é…æ–¹å¼": f"æ¨¡å‹ ({scope_desc})",
                                        "ç†ç”±": ai_res.get("reason")
                                    })
                                    
                                    if conf == "High": stats['high'] += 1
                                    elif conf == "Mid": stats['mid'] += 1
                                    else: stats['low'] += 1
                                    
                                    time.sleep(1.5) # å†·å´
                            
                            batch_results.append(base_res)
                            
                            processed_count += 1
                            progress_pct = processed_count / total_rem_rows
                            global_prog.progress(progress_pct)
                            
                        except Exception as e:
                            st.warning(f"è¡Œé”™è¯¯: {e}")
                    
                    # --- ğŸŒŸ æ‰¹æ¬¡å­˜æ¡£ç‚¹ ---
                    # æ¯è·‘å®Œä¸€ä¸ªåŒ…ï¼Œç«‹å³æ›´æ–°ç»“æœ
                    if batch_results:
                        df_batch = pd.DataFrame(batch_results)
                        final_accumulated = pd.concat([final_accumulated, df_batch], ignore_index=True)
                        
                        # å­˜å…¥ Session State (é˜²æ­¢æµè§ˆå™¨å´©æºƒåä¸¢å¤±å…¨éƒ¨)
                        st.session_state.final_result_df = final_accumulated
                        st.session_state.match_stats = stats
                        
                        # æ˜¾ç¤ºä¸´æ—¶ä¸‹è½½ (å¯é€‰ï¼Œç»™æåº¦è°¨æ…çš„ç”¨æˆ·)
                        # æ³¨æ„ï¼šst.download_button åœ¨å¾ªç¯ä¸­ä¸èƒ½ç›´æ¥ç‚¹å‡»ï¼Œè¿™é‡Œä¸»è¦èµ·å±•ç¤ºä½œç”¨ï¼Œæˆ–è€…ç”¨ st.empty æ›´æ–°
                        st.toast(f"âœ… ä»»åŠ¡åŒ… {batch_idx + 1} å®Œæˆï¼å·²è‡ªåŠ¨ä¿å­˜è¿›åº¦ã€‚", icon="ğŸ’¾")

                # å¾ªç¯ç»“æŸï¼Œæœ€ç»ˆè·³è½¬
                st.success("ğŸ‰ æ‰€æœ‰ä»»åŠ¡åŒ…å¤„ç†å®Œæˆï¼")
                st.rerun()
        
        else:
            # åªæœ‰å…¨å­—åŒ¹é…çš„æƒ…å†µ
            if st.button("âœ¨ ç›´æ¥ç”Ÿæˆç»“æœ", type="primary"):
                st.session_state.final_result_df = df_exact
                st.session_state.match_stats = {'exact': len(df_exact), 'high': 0, 'mid': 0, 'low': 0, 'no_match': 0}
                st.rerun()

# --- 4. ç»“æœå±•ç¤º ---
if st.session_state.final_result_df is not None:
    s = st.session_state.match_stats
    total = len(st.session_state.final_result_df)
    if total == 0: total = 1
    
    st.markdown("### ğŸ“Š åŒ¹é…ç»Ÿè®¡æŠ¥å‘Š")
    
    exact_val = s.get('exact', 0)
    exact_pct = exact_val / total
    
    model_done = s.get('high', 0) + s.get('mid', 0) + s.get('low', 0)
    model_pct = model_done / total
    model_denom = model_done if model_done > 0 else 1
    
    high_pct = s.get('high', 0) / model_denom
    mid_pct = s.get('mid', 0) / model_denom
    low_pct = s.get('low', 0) / model_denom
    
    c1, c2, c3, c4, c5 = st.columns(5)
    with c1: st.metric("ğŸ¯ å…¨å­—åŒ¹é…", f"{exact_val}", f"{exact_pct:.1%}")
    with c2: st.metric("ğŸ¤– æ¨¡å‹æ€»è®¡", f"{model_done}", f"{model_pct:.1%}")
    with c3: st.metric("ğŸ”¥ High", f"{s.get('high', 0)}", f"{high_pct:.1%} (of Model)")
    with c4: st.metric("âš–ï¸ Mid", f"{s.get('mid', 0)}", f"{mid_pct:.1%} (of Model)")
    with c5: st.metric("âš ï¸ Low", f"{s.get('low', 0)}", f"{low_pct:.1%} (of Model)")

    st.divider()
    
    def color_row(row):
        conf = row.get('ç½®ä¿¡åº¦')
        if conf == 'High': return ['background-color: #dcfce7'] * len(row)
        if conf == 'Mid': return ['background-color: #fef9c3'] * len(row)
        if conf == 'Low': return ['background-color: #fee2e2'] * len(row)
        return [''] * len(row)

    df_show = st.session_state.final_result_df
    st.dataframe(df_show.style.apply(color_row, axis=1), use_container_width=True)
    
    csv = df_show.to_csv(index=False).encode('utf-8-sig')
    st.download_button("ğŸ“¥ ä¸‹è½½å®Œæ•´ç»“æœ (å«æ‰€æœ‰æ‰¹æ¬¡)", csv, "linkmed_batch_result.csv", "text/csv", type="primary")
