import streamlit as st
import pandas as pd
import json
import time
import os
import gc
from google import genai
from google.genai import types
from rapidfuzz import process, fuzz 

# ================= 1. é…ç½®ä¸åˆå§‹åŒ– =================

st.set_page_config(page_title="LinkMed Matcher Ultimate", layout="wide", page_icon="ğŸ§¬")

try:
    FIXED_API_KEY = st.secrets["GENAI_API_KEY"]
except:
    FIXED_API_KEY = "" 

LOCAL_MASTER_FILE = "MDM_retail.xlsx"

# åˆå§‹åŒ– Session State
if 'uploader_key' not in st.session_state:
    st.session_state.uploader_key = str(time.time())
if 'final_result_df' not in st.session_state:
    st.session_state.final_result_df = None
if 'match_stats' not in st.session_state:
    st.session_state.match_stats = {}

# ================= 2. æ ¸å¿ƒå·¥å…·å‡½æ•° =================

def reset_app():
    st.session_state.final_result_df = None
    st.session_state.match_stats = {}
    st.session_state.uploader_key = str(time.time())
    st.rerun()

@st.cache_resource
def get_client():
    if not FIXED_API_KEY: return None
    return genai.Client(api_key=FIXED_API_KEY, http_options={'api_version': 'v1beta'})

def safe_generate(client, prompt, response_schema=None, retries=3):
    if client is None: return {"error": "API Key æœªé…ç½®"}
    wait_time = 2 
    for attempt in range(retries):
        try:
            config = types.GenerateContentConfig(
                response_mime_type="application/json",
                response_schema=response_schema
            )
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                contents=prompt,
                config=config
            )
            try:
                return json.loads(response.text)
            except json.JSONDecodeError:
                return {"error": "JSONè§£æå¤±è´¥", "raw": response.text}
        except Exception as e:
            if "429" in str(e) or "503" in str(e):
                if attempt < retries - 1:
                    time.sleep(wait_time * (2 ** attempt))
                    continue
            return {"error": str(e)}
    return {"error": "Max retries reached"}

@st.cache_resource(show_spinner=False)
def load_master_data():
    """åŠ è½½å¹¶å»ºç«‹å¤šç»´ç´¢å¼• (åœ°ç† + è¿é”)"""
    if os.path.exists(LOCAL_MASTER_FILE):
        try:
            gc.collect()
            if LOCAL_MASTER_FILE.endswith('.xlsx'):
                df = pd.read_excel(LOCAL_MASTER_FILE, engine='openpyxl')
            else:
                df = pd.read_csv(LOCAL_MASTER_FILE)
            
            # æ¸…æ´—
            if 'esid' in df.columns: df = df.drop_duplicates(subset=['esid'])
            cols_needed = ['æ ‡å‡†åç§°', 'çœ', 'å¸‚', 'åŒº', 'æœºæ„ç±»å‹', 'åœ°å€', 'è¿é”å“ç‰Œ'] 
            for col in cols_needed:
                if col not in df.columns: df[col] = '' 
            
            for c in cols_needed:
                df[c] = df[c].astype(str).str.strip()
            
            # 1. åœ°ç†ç´¢å¼•
            geo_index = {
                'province': df.groupby('çœ').groups,
                'city': df.groupby('å¸‚').groups,
                'district': df.groupby('åŒº').groups
            }
            
            # 2. è¿é”ç´¢å¼• (Chain Index) - ç”¨äºâ€œæ€»éƒ¨åŒ¹é…åˆ°æ‰€æœ‰é—¨åº—â€é€»è¾‘
            # å‡è®¾ä¸»æ•°æ®æœ‰ä¸€åˆ—å« 'è¿é”å“ç‰Œ' æˆ–ç±»ä¼¼ï¼Œå¦‚æœæ²¡æœ‰ï¼Œå¯ä»¥å°è¯•ä»æ ‡å‡†åç§°æå–ï¼ˆè¿™é‡Œç®€åŒ–ä¸ºå¿…é¡»æœ‰ä¸€åˆ—ï¼Œæˆ–è€…ç”¨æˆ·æŒ‡å®šåˆ—ï¼‰
            # ä¸ºäº†é€šç”¨æ€§ï¼Œæˆ‘ä»¬æš‚æ—¶å»ºç«‹ä¸€ä¸ªåŸºäº 'æ ‡å‡†åç§°' å‰ç¼€çš„ç®€å•å€’æ’ç´¢å¼•æ˜¯å¾ˆéš¾çš„ã€‚
            # è¿™é‡Œæˆ‘ä»¬ä¾èµ–ç”¨æˆ·ä¸Šä¼ æ—¶æŒ‡å®šçš„ 'è¿é”å“ç‰Œ' åˆ—ï¼Œæˆ–è€…ä¸»æ•°æ®é‡Œæœ‰çš„ 'è¿é”å“ç‰Œ' åˆ—ã€‚
            # å¦‚æœä¸»æ•°æ®æ²¡æœ‰ 'è¿é”å“ç‰Œ' åˆ—ï¼Œå»ºè®®åœ¨ Excel é‡Œå…ˆæ¸…æ´—å‡ºæ¥ã€‚
            
            chain_groups = {}
            if 'è¿é”å“ç‰Œ' in df.columns:
                # è¿‡æ»¤æ‰ç©ºçš„
                valid_chains = df[df['è¿é”å“ç‰Œ'].str.len() > 1]
                chain_groups = valid_chains.groupby('è¿é”å“ç‰Œ').groups
            
            return df, geo_index, chain_groups
        except Exception as e:
            st.error(f"è¯»å–ä¸»æ•°æ®é”™è¯¯: {e}")
            return pd.DataFrame(), {}, {}
    else:
        return pd.DataFrame(), {}, {}

def smart_map_columns(client, df_user):
    user_cols = df_user.columns.tolist()
    sample_data = df_user.head(3).to_markdown(index=False)
    
    prompt = f"""
    åˆ†æç”¨æˆ·æ•°æ®ï¼Œæ‰¾å‡ºä»¥ä¸‹å­—æ®µå¯¹åº”çš„åˆ—åã€‚
    ç”¨æˆ·åˆ—å: {user_cols}
    é¢„è§ˆ: {sample_data}
    
    ä»»åŠ¡ï¼šæ‰¾å‡ºä»¥ä¸‹åˆ—ï¼ˆå¦‚æœæ²¡æœ‰åˆ™è¿”å›nullï¼‰ï¼š
    1. name_col: è¯æˆ¿/ç»ˆç«¯åç§°
    2. chain_col: è¿é”/å“ç‰Œåç§° (å¦‚: æµ·ç‹æ˜Ÿè¾°ã€å¤§å‚æ—)
    3. prov_col: çœä»½
    4. city_col: åŸå¸‚
    5. dist_col: åŒº/å¿
    6. addr_col: è¯¦ç»†åœ°å€
    
    è¾“å‡º JSON: {{ "name_col": "...", "chain_col": "...", "prov_col": "...", "city_col": "...", "dist_col": "...", "addr_col": "..." }}
    """
    res = safe_generate(client, prompt)
    if isinstance(res, list): res = res[0] if res else {}
    return res

def get_candidates_hybrid(search_name, chain_name, df_master, geo_index, chain_groups, user_row, mapping):
    """
    ğŸŒŸ æ··åˆæ£€ç´¢é€»è¾‘ï¼šåœ°ç†æ¼æ–— + è¿é”ä¸‹é’»
    """
    # 1. ç¡®å®šåœ°ç†èŒƒå›´ç´¢å¼•
    u_prov = str(user_row[mapping['prov']]) if mapping['prov'] and pd.notna(user_row[mapping['prov']]) else ''
    u_city = str(user_row[mapping['city']]) if mapping['city'] and pd.notna(user_row[mapping['city']]) else ''
    u_dist = str(user_row[mapping['dist']]) if mapping['dist'] and pd.notna(user_row[mapping['dist']]) else ''
    
    geo_indices = set()
    scope_level = "Global"

    if u_dist and u_dist in geo_index['district']:
        geo_indices = set(geo_index['district'][u_dist])
        scope_level = f"District ({u_dist})"
    elif u_city and u_city in geo_index['city']:
        geo_indices = set(geo_index['city'][u_city])
        scope_level = f"City ({u_city})"
    elif u_prov and u_prov in geo_index['province']:
        geo_indices = set(geo_index['province'][u_prov])
        scope_level = f"Province ({u_prov})"
    else:
        # å…¨å±€æ¨¡å¼ï¼Œç¨å¾®å±é™©ï¼Œä½†å¦‚æœæ²¡æœ‰åœ°ç†ä¿¡æ¯åªèƒ½è¿™æ ·
        geo_indices = set(df_master.index)
        scope_level = "Global (No Geo)"

    candidates_indices = set()

    # 2. ç­–ç•¥ A: è¿é”ä¸‹é’» (Chain Drill-Down) - å¯¹åº”éœ€æ±‚ 1
    # å¦‚æœç”¨æˆ·æä¾›äº†è¿é”åï¼Œä¸”åœ¨ä¸»æ•°æ®ä¸­æœ‰è¯¥è¿é”çš„ç´¢å¼•
    # æˆ‘ä»¬å¼ºåˆ¶æŠŠè¯¥åœ°ç†èŒƒå›´å†…çš„ *è¯¥è¿é”æ‰€æœ‰é—¨åº—* éƒ½åŠ è¿›æ¥
    
    # å°è¯•ä»ç”¨æˆ·åˆ—è·å–è¿é”åï¼Œæˆ–è€…ä»åå­—ä¸­æå–ï¼ˆç®€å•åŒ…å«åˆ¤æ–­ï¼‰
    # è¿™é‡Œä½¿ç”¨ç”¨æˆ·æä¾›çš„ chain_name å‚æ•°
    if chain_name and chain_name in chain_groups:
        chain_store_indices = set(chain_groups[chain_name])
        # å–äº¤é›†ï¼šè¯¥è¿é” && è¯¥åœ°ç†èŒƒå›´
        valid_chain_stores = chain_store_indices.intersection(geo_indices)
        candidates_indices.update(valid_chain_stores)
        if len(valid_chain_stores) > 0:
            scope_level += " + Chain Drill-down"

    # 3. ç­–ç•¥ B: æ¨¡ç³Šæœç´¢ (Fuzzy Search)
    # åœ¨åœ°ç†èŒƒå›´å†…è¿›è¡Œæ¨¡ç³Šæœç´¢
    # ä¸ºäº†é€Ÿåº¦ï¼Œå¦‚æœ geo_indices å¤ªå¤§ï¼ˆ>2000ï¼‰ï¼Œæˆ‘ä»¬å¯èƒ½åªæœä¸€éƒ¨åˆ†ï¼Œæˆ–è€… RapidFuzz è¶³å¤Ÿå¿«
    
    if geo_indices:
        # æå–å½“å‰èŒƒå›´å†…çš„åå­—å­—å…¸
        current_scope_df = df_master.loc[list(geo_indices)]
        choices = current_scope_df['æ ‡å‡†åç§°'].fillna('').astype(str).to_dict()
        
        # æ¨¡ç³Šæœç´¢å‰ 5-8 å
        results = process.extract(search_name, choices, limit=8, scorer=fuzz.WRatio)
        for r in results:
            candidates_indices.add(r[2]) # r[2] is index

    return list(candidates_indices), scope_level

def ai_match_row_expert(client, user_row, search_name, chain_name, scope_level, candidates_df):
    
    # å‡†å¤‡ Prompt æ•°æ®
    cols_to_keep = ['esid', 'æ ‡å‡†åç§°', 'æœºæ„ç±»å‹', 'çœ', 'å¸‚', 'åŒº', 'åœ°å€', 'è¿é”å“ç‰Œ']
    valid_cols = [c for c in cols_to_keep if c in candidates_df.columns]
    candidates_json = candidates_df[valid_cols].to_json(orient="records", force_ascii=False)
    
    # ğŸŒŸğŸŒŸğŸŒŸ æ ¸å¿ƒ Prompt ä¼˜åŒ– ğŸŒŸğŸŒŸğŸŒŸ
    prompt = f"""
    ã€è§’è‰²ã€‘ä½ æ˜¯ä¸€ä¸ªç²¾é€šåœ°ç†ä½ç½®çš„ä¸»æ•°æ®åŒ¹é…ä¸“å®¶ã€‚
    
    ã€å¾…åŒ¹é…è¾“å…¥ã€‘
    - æœç´¢åç§°: "{search_name}"
    - è¯†åˆ«åˆ°çš„è¿é”å“ç‰Œ: "{chain_name}"
    - åœ°ç†èŒƒå›´: {scope_level}
    - åŸå§‹å®Œæ•´è¡Œ: {user_row.to_json(force_ascii=False)}
    
    ã€å€™é€‰ä¸»æ•°æ®åˆ—è¡¨ã€‘ (å·²é™åˆ¶åœ¨ç›¸åŒåœ°ç†èŒƒå›´å†…):
    {candidates_json}
    
    ã€åŒ¹é…å†³ç­–æ€ç»´é“¾ã€‘:
    1. **è¿é”æ€»éƒ¨é™·é˜±**: 
       - å¦‚æœå€™é€‰åˆ—è¡¨ä¸­åŒ…å«â€œæ€»éƒ¨â€ã€â€œæ€»å…¬å¸â€ã€â€œè‚¡ä»½æœ‰é™å…¬å¸â€ç­‰éé—¨åº—ç±»å‹çš„è®°å½•ï¼Œ**é™¤éè¾“å…¥æ˜ç¡®æŒ‡æ˜æ˜¯æ€»éƒ¨ï¼Œå¦åˆ™ä¸è¦åŒ¹é…å®ƒä»¬**ã€‚
       - ç”¨æˆ·çš„çœŸå®æ„å›¾é€šå¸¸æ˜¯å¯»æ‰¾è¯¥è¿é”åœ¨å½“åœ°çš„**å…·ä½“é—¨åº—**ã€‚
       - å¦‚æœæ— æ³•ç¡®å®šå…·ä½“é—¨åº—ï¼Œå®å¯è¿”å› Low ç½®ä¿¡åº¦ï¼Œä¹Ÿä¸è¦é”™è¯¯åŒ¹é…åˆ°æ€»éƒ¨ã€‚
    
    2. **åœ°åäº¤å‰éªŒè¯ (Cross-Field Check)**:
       - ç”¨æˆ·çš„â€œæœç´¢åç§°â€ä¸­å¯èƒ½åŒ…å«äº†åœ°åæˆ–è·¯åï¼ˆä¾‹å¦‚è¾“å…¥ï¼šâ€œæµ·ç‹æ˜Ÿè¾°å—å±±åº—â€ æˆ– â€œæµ·ç‹æ˜Ÿè¾°äººæ°‘è·¯â€ï¼‰ã€‚
       - è¯·åŠ¡å¿…æ£€æŸ¥å€™é€‰æ•°æ®çš„**ã€åœ°å€ã€‘**åˆ—ï¼
       - å¦‚æœå€™é€‰çš„ã€æ ‡å‡†åç§°ã€‘ä¸åŒ¹é…ï¼Œä½†å…¶ã€åœ°å€ã€‘åŒ…å«äº†è¾“å…¥åç§°ä¸­çš„è·¯å/åœ°åï¼Œè¿™æ˜¯ä¸€ä¸ªæå¼ºçš„åŒ¹é…ä¿¡å· (High Confidence)ã€‚
    
    3. **åç§°æ„å»º**:
       - å¦‚æœè¾“å…¥æ˜¯ "è¿é”å + åœ°å" (å¦‚ "å¤§å‚æ— ä¸œé—¨")ï¼Œè¯·å¯»æ‰¾åç§°æˆ–åœ°å€ä¸­åŒ…å« "ä¸œé—¨" çš„è¯¥è¿é”é—¨åº—ã€‚
    
    ã€è¾“å‡º JSON æ ¼å¼ã€‘
    {{
        "match_esid": "åŒ¹é…åˆ°çš„ESID (æ— åŒ¹é…å¡«null)",
        "match_name": "åŒ¹é…åˆ°çš„æ ‡å‡†åç§°",
        "match_type": "æœºæ„ç±»å‹",
        "confidence": "High/Low",
        "reason": "è¯·æ˜ç¡®è¯´æ˜ï¼šæ˜¯å¦é€šè¿‡åœ°å€äº¤å‰éªŒè¯å‘½ä¸­äº†ï¼Ÿæ˜¯å¦é¿å¼€äº†æ€»éƒ¨ï¼Ÿ"
    }}
    """
    return safe_generate(client, prompt)

# ================= 3. é¡µé¢ UI =================

st.markdown("""
    <style>
    .stApp {background-color: #F8F9FA;}
    .stat-card {background: #ffffff; padding: 15px; border-radius: 8px; border: 1px solid #e5e7eb; box-shadow: 0 1px 2px rgba(0,0,0,0.05);}
    .big-num {font-size: 24px; font-weight: bold; color: #1e40af;}
    .sub-text {font-size: 14px; color: #6b7280;}
    .success-box {background-color: #dcfce7; color: #166534; padding: 10px; border-radius: 5px; border: 1px solid #bbf7d0; margin-bottom: 10px;}
    .info-box {background-color: #e0f2fe; color: #075985; padding: 10px; border-radius: 5px; border: 1px solid #bae6fd; margin-bottom: 10px;}
    </style>
    <div style="font-size: 26px; font-weight: bold; color: #1E3A8A; margin-bottom: 20px;">
    ğŸ§¬ LinkMed Matcher (Expert Logic)
    </div>
""", unsafe_allow_html=True)

client = get_client()

# åŠ è½½æ•°æ® & ç´¢å¼•
df_master, geo_index, chain_groups = pd.DataFrame(), {}, {}
if os.path.exists(LOCAL_MASTER_FILE):
    with st.spinner(f"æ­£åœ¨åŠ è½½ä¸»æ•°æ®å¹¶æ„å»ºå¤šç»´ç´¢å¼•..."):
        df_master, geo_index, chain_groups = load_master_data()
else:
    st.warning(f"âš ï¸ æ–‡ä»¶ç¼ºå¤±: `{LOCAL_MASTER_FILE}`")

# --- Sidebar ---
with st.sidebar:
    st.header("ğŸ—„ï¸ æ§åˆ¶å°")
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºé‡ç½®", type="secondary", use_container_width=True):
        reset_app()
    if not df_master.empty:
        st.success(f"ä¸»æ•°æ®: {len(df_master)} æ¡")
        st.caption(f"å·²è¯†åˆ«è¿é”å“ç‰Œæ•°: {len(chain_groups)}")

# --- ä¸»æµç¨‹ ---
if st.session_state.final_result_df is None:
    st.markdown("### ğŸ“‚ 1. ä¸Šä¼ æ•°æ®")
    uploaded_file = st.file_uploader("Excel/CSV", type=['xlsx', 'csv'], key=st.session_state.uploader_key)

    if uploaded_file and not df_master.empty:
        if uploaded_file.name.endswith('.csv'): df_user = pd.read_csv(uploaded_file)
        else: df_user = pd.read_excel(uploaded_file)
        
        # --- 2. å­—æ®µæ˜ å°„ ---
        st.markdown("### ğŸ¤– 2. å­—æ®µæ˜ å°„")
        if 'map_config' not in st.session_state or st.session_state.get('last_file') != uploaded_file.name:
            with st.spinner("AI æ­£åœ¨åˆ†æè¡¨å¤´..."):
                st.session_state.map_config = smart_map_columns(client, df_user)
                st.session_state.last_file = uploaded_file.name
        
        map_res = st.session_state.map_config
        cols = df_user.columns.tolist()
        
        c1, c2, c3 = st.columns(3)
        def get_idx(key): return cols.index(map_res.get(key)) if map_res.get(key) in cols else 0
        
        with c1:
            col_name = st.selectbox("ğŸ“ è¯æˆ¿åç§°", cols, index=get_idx('name_col'))
            col_chain = st.selectbox("ğŸ”— è¿é”åç§° (å¯é€‰)", [None]+cols, index=cols.index(map_res['chain_col'])+1 if map_res.get('chain_col') in cols else 0)
        with c2:
            col_prov = st.selectbox("ğŸ—ºï¸ çœä»½ (å¯é€‰)", [None]+cols, index=cols.index(map_res['prov_col'])+1 if map_res.get('prov_col') in cols else 0)
            col_city = st.selectbox("ğŸ™ï¸ åŸå¸‚ (å¯é€‰)", [None]+cols, index=cols.index(map_res['city_col'])+1 if map_res.get('city_col') in cols else 0)
        with c3:
            col_dist = st.selectbox("ğŸ˜ï¸ åŒºå¿ (å¯é€‰)", [None]+cols, index=cols.index(map_res['dist_col'])+1 if map_res.get('dist_col') in cols else 0)
            col_addr = st.selectbox("ğŸ  è¯¦ç»†åœ°å€ (å¯é€‰)", [None]+cols, index=cols.index(map_res['addr_col'])+1 if map_res.get('addr_col') in cols else 0)

        mapping = {
            'prov': col_prov, 'city': col_city, 'dist': col_dist, 
            'addr': col_addr, 'chain': col_chain, 'name': col_name
        }

        # --- 3. é¢„å¤„ç†åˆ†æµ ---
        st.markdown("### âš¡ 3. é¢„å¤„ç†ä¸æ‰§è¡Œ")
        
        master_exact = df_master.drop_duplicates(subset=['æ ‡å‡†åç§°']).set_index('æ ‡å‡†åç§°').to_dict('index')
        exact_rows_data = []
        remaining_indices = []
        
        for idx, row in df_user.iterrows():
            raw_name = str(row[col_name]).strip()
            chain_name = str(row[col_chain]).strip() if col_chain and pd.notna(row[col_chain]) else ""
            
            # æ„å»ºç”¨äºå…¨å­—åŒ¹é…çš„åç§°
            search_name = raw_name
            if chain_name and chain_name not in raw_name:
                search_name = f"{chain_name} {raw_name}"
            
            if search_name in master_exact:
                m = master_exact[search_name]
                res = row.to_dict()
                res.update({
                    "åŒ¹é…ESID": m.get('esid'),
                    "åŒ¹é…æ ‡å‡†å": search_name,
                    "æœºæ„ç±»å‹": m.get('æœºæ„ç±»å‹'),
                    "ç½®ä¿¡åº¦": "High",
                    "åŒ¹é…æ–¹å¼": "å…¨å­—åŒ¹é…",
                    "ç†ç”±": "ç²¾ç¡®å‘½ä¸­"
                })
                exact_rows_data.append(res)
            else:
                remaining_indices.append(idx)
        
        df_exact_pre = pd.DataFrame(exact_rows_data)
        df_remaining = df_user.loc[remaining_indices].copy()
        
        count_exact = len(df_exact_pre)
        count_rem = len(df_remaining)
        
        st.markdown(f"""
        <div class="success-box">âœ… <b>å·²è‡ªåŠ¨å‘½ä¸­ {count_exact} è¡Œ</b></div>
        <div class="info-box">â³ <b>å‰©ä½™ {count_rem} è¡Œ</b> å¾…æ¨¡å‹å¤„ç†ï¼ˆå·²å¯ç”¨æ€»éƒ¨è§„é¿ç®—æ³•ï¼‰</div>
        """, unsafe_allow_html=True)
        
        if count_rem > 0:
            btn_text = f"ğŸš€ å¼€å§‹æ·±åº¦åŒ¹é…å‰©ä½™ {count_rem} è¡Œ"
            btn_type = "primary"
        else:
            btn_text = "âœ¨ ç›´æ¥ç”Ÿæˆç»“æœ"
            btn_type = "secondary"

        if st.button(btn_text, type=btn_type):
            
            ai_results_data = []
            stats = {'total': len(df_user), 'exact': count_exact, 'high': 0, 'low': 0, 'no_match': 0}
            
            if count_rem > 0:
                prog = st.progress(0)
                status = st.empty()
                
                for i, (orig_idx, row) in enumerate(df_remaining.iterrows()):
                    try:
                        raw_name = str(row[col_name]).strip()
                        chain_name = str(row[col_chain]).strip() if col_chain and pd.notna(row[col_chain]) else ""
                        
                        search_name = raw_name
                        if chain_name and chain_name not in raw_name:
                            search_name = f"{chain_name} {raw_name}"

                        # ğŸŒŸ è°ƒç”¨æ··åˆæ£€ç´¢ (Hybrid Retrieval)
                        indices, scope = get_candidates_hybrid(search_name, chain_name, df_master, geo_index, chain_groups, row, mapping)
                        
                        base_res = row.to_dict()
                        
                        if not indices:
                            base_res.update({
                                "åŒ¹é…ESID": None, "åŒ¹é…æ ‡å‡†å": None, "æœºæ„ç±»å‹": None,
                                "ç½®ä¿¡åº¦": "Low", "åŒ¹é…æ–¹å¼": "æ— ç»“æœ", "ç†ç”±": "æ— ç›¸ä¼¼å€™é€‰"
                            })
                            stats['no_match'] += 1
                        else:
                            candidates = df_master.loc[indices].copy()
                            # ğŸŒŸ è°ƒç”¨ä¸“å®¶çº§ Prompt
                            ai_res = ai_match_row_expert(client, row, search_name, chain_name, scope, candidates)
                            
                            if isinstance(ai_res, list): ai_res = ai_res[0] if ai_res else {}
                            
                            conf = ai_res.get("confidence", "Low")
                            base_res.update({
                                "åŒ¹é…ESID": ai_res.get("match_esid"),
                                "åŒ¹é…æ ‡å‡†å": ai_res.get("match_name"),
                                "æœºæ„ç±»å‹": ai_res.get("match_type"),
                                "ç½®ä¿¡åº¦": conf,
                                "åŒ¹é…æ–¹å¼": f"æ¨¡å‹åŒ¹é…",
                                "ç†ç”±": ai_res.get("reason")
                            })
                            
                            if conf == "High": stats['high'] += 1
                            else: stats['low'] += 1
                            
                            time.sleep(1.5)
                            
                        ai_results_data.append(base_res)
                        prog.progress((i+1)/count_rem)
                        status.text(f"Processing ({i+1}/{count_rem}): {search_name}")
                        
                    except Exception as e:
                        st.error(f"Error at index {orig_idx}: {e}")
                        break
            
            if ai_results_data:
                df_ai_res = pd.DataFrame(ai_results_data)
                df_final = pd.concat([df_exact_pre, df_ai_res], ignore_index=True)
            else:
                df_final = df_exact_pre
            
            st.session_state.final_result_df = df_final
            st.session_state.match_stats = stats
            st.rerun()

# --- 4. ç»“æœå±•ç¤º ---
if st.session_state.final_result_df is not None:
    s = st.session_state.match_stats
    total = s.get('total', 1)
    if total == 0: total = 1
    
    st.markdown("### ğŸ“Š åŒ¹é…ç»Ÿè®¡æŠ¥å‘Š")
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.markdown(f"""
        <div class="stat-card">
            <div class="sub-text">ğŸ¯ å…¨å­—åŒ¹é…</div>
            <div class="big-num">{s['exact']} è¡Œ</div>
            <div style="color:green; font-weight:bold;">{s['exact']/total:.1%}</div>
        </div>""", unsafe_allow_html=True)
    with col2:
        model_done = s['high'] + s['low']
        st.markdown(f"""
        <div class="stat-card">
            <div class="sub-text">ğŸ¤– æ¨¡å‹å¤„ç†</div>
            <div class="big-num">{model_done} è¡Œ</div>
            <div style="color:blue; font-weight:bold;">{model_done/total:.1%}</div>
        </div>""", unsafe_allow_html=True)
    with col3:
        st.markdown(f"""
        <div class="stat-card">
            <div class="sub-text">ğŸ”¥ High ç½®ä¿¡åº¦</div>
            <div class="big-num">{s['high']} è¡Œ</div>
            <div class="sub-text">å æ¨¡å‹: {s['high']/model_done:.1% if model_done else 0}</div>
        </div>""", unsafe_allow_html=True)
    with col4:
        st.markdown(f"""
        <div class="stat-card">
            <div class="sub-text">âš ï¸ Low ç½®ä¿¡åº¦</div>
            <div class="big-num">{s['low']} è¡Œ</div>
            <div class="sub-text">å æ¨¡å‹: {s['low']/model_done:.1% if model_done else 0}</div>
        </div>""", unsafe_allow_html=True)

    st.divider()
    
    def color_row(row):
        if row['åŒ¹é…æ–¹å¼'] == 'å…¨å­—åŒ¹é…': return ['background-color: #dcfce7'] * len(row)
        if row.get('ç½®ä¿¡åº¦') == 'High': return ['background-color: #e0f2fe'] * len(row)
        return [''] * len(row)

    df_show = st.session_state.final_result_df
    st.dataframe(df_show.style.apply(color_row, axis=1), use_container_width=True)
    
    csv = df_show.to_csv(index=False).encode('utf-8-sig')
    st.download_button("ğŸ“¥ ä¸‹è½½å®Œæ•´æŠ¥å‘Š", csv, "linkmed_expert_result.csv", "text/csv", type="primary")
